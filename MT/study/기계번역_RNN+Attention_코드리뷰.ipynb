{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "기계번역 RNN+Attention 코드리뷰.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bcmin1018/NLP/blob/main/MT/study/%EA%B8%B0%EA%B3%84%EB%B2%88%EC%97%AD_RNN%2BAttention_%EC%BD%94%EB%93%9C%EB%A6%AC%EB%B7%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference Link\n",
        "* https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html"
      ],
      "metadata": {
        "id": "g42HU4UJYnNM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv0tsAGZD_qX",
        "outputId": "5dfbdcb4-f9aa-47c5-b34e-6891d735bafd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DGeE1UY8nAhlYwLIIyNRHJ-SlVaxcURU\n",
            "To: /content/train_en_ko.csv\n",
            "100% 145k/145k [00:00<00:00, 80.3MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GPUgAlDKn0Wk1TqILJwzqeS84eBiOngU\n",
            "To: /content/valid_en_ko.csv\n",
            "100% 72.5k/72.5k [00:00<00:00, 69.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1DGeE1UY8nAhlYwLIIyNRHJ-SlVaxcURU\n",
        "!gdown --id 1GPUgAlDKn0Wk1TqILJwzqeS84eBiOngU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CahV8JRlEOlK",
        "outputId": "92788975-609d-4718-8d70-c10093146df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 50.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.2.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "import nltk\n",
        "import codecs\n",
        "import csv\n",
        "from konlpy.tag import Okt\n",
        "import torch.nn.functional as F\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luaFE46MEEWL",
        "outputId": "867f17ff-7eef-40cf-d3f4-c752f1c77608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv(\"train_en_ko.csv\", header = None, names=['english', 'korean'])"
      ],
      "metadata": {
        "id": "S55eH2y8EGEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "xRpASvugnpLZ",
        "outputId": "06f60e16-9314-421e-b78e-e60a71846539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                english  \\\n",
              "0     Through the snow and sleet and hail, through t...   \n",
              "1     ever faithful, ever true, nothing stops him, h...   \n",
              "2           Look out for Mr Stork That persevering chap   \n",
              "3        He'll come along and drop a bundle in your lap   \n",
              "4       You may be poor or rich It doesn't matter which   \n",
              "...                                                 ...   \n",
              "1995                       I'd rather kiss a tarantula!   \n",
              "1996                              -You don't mean that!   \n",
              "1997                                        -l don't--?   \n",
              "1998            Joe, bring me a tarantula. Now listen--   \n",
              "1999  Stop that chitchat, you lovebirds. Let's get a...   \n",
              "\n",
              "                                                 korean  \n",
              "0     폭설이 내리고 우박, 진눈깨비가 퍼부어도 눈보라가 몰아쳐도 강풍이 불고 비바람이 휘...  \n",
              "1                  우리의 한결같은 심부름꾼 황새 아저씨 가는 길을 그 누가 막으랴!  \n",
              "2                                         황새 아저씨를 기다리세요  \n",
              "3                                        찾아와 선물을 주실 거예요  \n",
              "4                                    가난하든 부자이든 상관이 없답니다  \n",
              "...                                                 ...  \n",
              "1995                                     차라리 독거미와 키스하겠다  \n",
              "1996                                            못 할 거면서  \n",
              "1997                                               못 해?  \n",
              "1998                                  독거미 가져 와 잘 들어, 리나  \n",
              "1999                             사랑싸움은 그만 하고 다음 장면을 찍자구  \n",
              "\n",
              "[2000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-91eb5479-a44b-4fa5-8c0b-b45d593a47ca\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english</th>\n",
              "      <th>korean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Through the snow and sleet and hail, through t...</td>\n",
              "      <td>폭설이 내리고 우박, 진눈깨비가 퍼부어도 눈보라가 몰아쳐도 강풍이 불고 비바람이 휘...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ever faithful, ever true, nothing stops him, h...</td>\n",
              "      <td>우리의 한결같은 심부름꾼 황새 아저씨 가는 길을 그 누가 막으랴!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Look out for Mr Stork That persevering chap</td>\n",
              "      <td>황새 아저씨를 기다리세요</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>He'll come along and drop a bundle in your lap</td>\n",
              "      <td>찾아와 선물을 주실 거예요</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You may be poor or rich It doesn't matter which</td>\n",
              "      <td>가난하든 부자이든 상관이 없답니다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>I'd rather kiss a tarantula!</td>\n",
              "      <td>차라리 독거미와 키스하겠다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>-You don't mean that!</td>\n",
              "      <td>못 할 거면서</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>-l don't--?</td>\n",
              "      <td>못 해?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>Joe, bring me a tarantula. Now listen--</td>\n",
              "      <td>독거미 가져 와 잘 들어, 리나</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>Stop that chitchat, you lovebirds. Let's get a...</td>\n",
              "      <td>사랑싸움은 그만 하고 다음 장면을 찍자구</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91eb5479-a44b-4fa5-8c0b-b45d593a47ca')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-91eb5479-a44b-4fa5-8c0b-b45d593a47ca button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-91eb5479-a44b-4fa5-8c0b-b45d593a47ca');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 언어사전 만들기"
      ],
      "metadata": {
        "id": "nwxtx2Iawdsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 사전 내 특수 단어의 index를 각각 미리 정의합니다.\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2\n",
        "\n",
        "class LangDic:\n",
        "    # 언어마다 단어 사전을 정의합니다.\n",
        "    def __init__(self, name, tokenizer):\n",
        "        # 클래스의 첫 시작인 함수입니다. 여기서 모델에 필요한 여러 변수들을 정의합니다.\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"UNK\"}\n",
        "        self.n_words = 2\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        # 문장을 받아 문장에서 단어를 확인합니다.\n",
        "        for word in self.tokenizer(sentence):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        # 단어를 보고 그 단어가 사전에 존재하는지 아닌지를 살펴봅니다.\n",
        "        # 존재하지 않는 경우 단어를 사전에 등록합니다.\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def sentence2tensor(self, sentence, max_len):\n",
        "        # 가지고 있는 사전을 바탕으로 문장을 tensor의 형태로 바꿉니다.\n",
        "        # tensor 내 들어있는 값은 단어 index이며 이를 통해 모델의 임베딩에 입력으로 줄 수 있습니다.\n",
        "        indexes = list()\n",
        "        for word in self.tokenizer(sentence):\n",
        "            try:\n",
        "                indexes.append(self.word2index[word])\n",
        "            except KeyError:\n",
        "                indexes.append(UNK_token)\n",
        "\n",
        "        indexes.append(EOS_token)\n",
        "        len_sen = len(indexes)\n",
        "        if len_sen > max_len:\n",
        "            indexes = indexes[:max_len-1]\n",
        "            indexes.append(EOS_token)\n",
        "            len_sen = max_len\n",
        "\n",
        "        index_tensor = torch.tensor(indexes)\n",
        "        return index_tensor, len_sen"
      ],
      "metadata": {
        "id": "yzDmfu0ZxOhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dic(dataset_path):\n",
        "    # 사전을 만드는 함수입니다.\n",
        "    data_pairs = load_file(dataset_path)\n",
        "\n",
        "    # 영어의 경우 NLTK tokenizer를\n",
        "    # 한국어의 경우 Konlpy 내 Open Korean Text tokenizer를 이용합니다.\n",
        "    eng_tokenizer = word_tokenize\n",
        "    kor_tokenizer = Okt().morphs\n",
        "\n",
        "    eng_dic = LangDic('en', eng_tokenizer)\n",
        "    kor_dic = LangDic('ko', kor_tokenizer)\n",
        "\n",
        "    for eng_sen, kor_sen in data_pairs:\n",
        "        eng_dic.add_sentence(eng_sen)\n",
        "        kor_dic.add_sentence(kor_sen)\n",
        "\n",
        "    return eng_dic, kor_dic\n",
        "\n",
        "def load_file(dataset_path):\n",
        "    # 데이터를 읽는 함수입니다.\n",
        "    data_pairs = list()\n",
        "    # 데이터 파일의 내용을 불러와 영어 문장과 한국어 문장을 모아 리스트에 넣습니다.\n",
        "    with codecs.open(dataset_path, \"r\", \"utf-8\") as csv_f:\n",
        "        csv_reader = csv.reader(csv_f)\n",
        "        for one_row in csv_reader:\n",
        "            data_pairs.append(one_row)\n",
        "\n",
        "    return data_pairs"
      ],
      "metadata": {
        "id": "qDhR79mP-9_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file_path = \"./train_en_ko.csv\"\n",
        "valid_file_path = \"./valid_en_ko.csv\""
      ],
      "metadata": {
        "id": "b2-9EF1z-5pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_dic, kor_dic = make_dic(dataset_path=train_file_path)"
      ],
      "metadata": {
        "id": "NFJL63AB-02R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 예제 데이터 샘플\n",
        "* 먼저 실제 데이터를 준비된 언어 사전을 기반으로 숫자로 치환합니다. 예를 들어 \"나는 학교에 갑니다.\"와 같은 문자로된 데이터가 5, 1, 3 과 같은 숫자로 변형됩니다. 숫자로 변형되는 기준은 언어 사전에 따라 다르게 나타납니다. 치환된 숫자는 학습 모델에 맞게 tensor 형태로 변환됩니다."
      ],
      "metadata": {
        "id": "0rS1R2T6gQjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MTDataset(Dataset):\n",
        "    # pytorch로 데이터를 불러오기 위해서 Dataset 클래스를 상속받아 새로운 클래스를 만듭니다.\n",
        "    def __init__(self, data_pairs, eng_dic, kor_dic, max_len):\n",
        "        super(MTDataset, self).__init__()\n",
        "\n",
        "        # 데이터를 파일로부터 읽어 이를 전달 받습니다.\n",
        "        self.max_len = max_len\n",
        "        self.pair_data = list()\n",
        "\n",
        "        # 데이터 내 문장을 미리 정의한 사전에 기반하여 tensor로 바꿉니다.\n",
        "        for eng_sen, kor_sen in data_pairs:\n",
        "            eng_sen_words, eng_sen_len = eng_dic.sentence2tensor(eng_sen, max_len)\n",
        "            kor_sen_words, kor_sen_len = kor_dic.sentence2tensor(kor_sen, max_len)\n",
        "            self.pair_data.append((eng_sen_words, eng_sen_len, kor_sen_words, kor_sen_len))\n",
        "\n",
        "        self.data_len = len(self.pair_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # idx번째 데이터를 반환합니다.\n",
        "        eng_sen_words, eng_sen_len, kor_sen_words, kor_sen_len = self.pair_data[idx]\n",
        "\n",
        "        return eng_sen_words, eng_sen_len, kor_sen_words, kor_sen_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_len"
      ],
      "metadata": {
        "id": "kAk6NB1Hy_IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tuple = train.to_records(index=False)\n",
        "for eng_sen, kor_sen in train_tuple:\n",
        "  eng_dic.add_sentence(eng_sen)\n",
        "  kor_dic.add_sentence(kor_sen)"
      ],
      "metadata": {
        "id": "uuIK1pC4AdGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = MTDataset(train_tuple, eng_dic, kor_dic, 10)\n",
        "train_loader = DataLoader(ds, batch_size = 1)"
      ],
      "metadata": {
        "id": "1cVurRfo0Fhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_tensor, input_length, target_tensor, target_length in train_loader:\n",
        "  input_tensor = input_tensor[0]\n",
        "  target_tensor = target_tensor[0]\n",
        "  input_length = input_length\n",
        "  target_length = target_length\n",
        "  break"
      ],
      "metadata": {
        "id": "TEtAFWgMAmyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 최종적으로 입력/출력 텐서값과 길이를 출력하였습니다. 영어를 한국어로 번역하기 때문에 입력은 영어를 뜻하고 출력은 한국어를 의미합니다."
      ],
      "metadata": {
        "id": "qTbi9K4chU4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"입력 텐서 값 \", input_tensor)\n",
        "print(\"출력 텐서 값 \", target_tensor)\n",
        "print(\"입력 텐서 길이 \", input_length)\n",
        "print(\"출력 텐서 길이 \", target_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5-ZJBWxAqpY",
        "outputId": "6c5e68f5-9497-4fba-f328-12e938611975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 텐서 값  tensor([2, 3, 4, 5, 6, 5, 7, 8, 9, 1])\n",
            "출력 텐서 값  tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10,  1])\n",
            "입력 텐서 길이  tensor([10])\n",
            "출력 텐서 길이  tensor([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Encoder\n",
        "\n",
        "Sequence to sequence 모델에 Attention을 적용한다는 의미는 encoder의 입력에 대한 각각의 output, 즉 모든 hidden_state 정보를 이용한다는 것이다. 각 hidden_state를 사용하여 어떤 단어에 집중 할지에 대해 파악한다.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/101251439/167293777-526b0adf-24be-4b3c-84b4-6663afbbce76.png)"
      ],
      "metadata": {
        "id": "vFpMzMi6_g01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Encoder에 필요한 입력 값을 만듭니다.\n",
        "  * input_size: encoder의 입력 차원 (영어 언어 사전 길이에 해당된다., 한 문자를 원핫 인코딩하면 단어 사전 길이 만큼의 차원을 갖게 되기 때문이다.)\n",
        "\n"
      ],
      "metadata": {
        "id": "2fGA40KJBiNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eng_dic_size = eng_dic.n_words\n",
        "input_size = eng_dic_size\n",
        "hidden_size = 256\n",
        "max_len = 10\n",
        "print(\"encoder_input_size: \", input_size)\n",
        "print(\"hidden_size: \", hidden_size)\n",
        "print(\"max_len :\", max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-Wwwzk_klkP",
        "outputId": "9bd31890-b2b8-4218-8468-5ccb3292e12b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_input_size:  2506\n",
            "hidden_size:  256\n",
            "max_len : 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Encoder 동작 방식 : 문자 입력 -> 임베딩 -> RNN 순서 -> context vector 및 모든 hidden state 생성\n",
        "  1. encoder_hidden: 최종 output(context vector)를 담을 배열(텐서) 생성\n",
        "  2. encoder_outputs: 모든 output을 담을 배열 (텐서) 생성 \n",
        "  3. embedding layer 생성\n",
        "  4. RNN layer 생성\n",
        "  5. 영어 데이터 입력하기\n",
        "  6. encoder_hidden, encoder_outputs 출력하기"
      ],
      "metadata": {
        "id": "xhC5n7DREJze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder의 최종 output인 context vecotor을 담을 3차원 텐서 만들기 간단하게 0으로 만들기\n",
        "encoder_hidden = torch.zeros(1,1,256)\n",
        "encoder_hidden.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMI3QFGY2YO6",
        "outputId": "4e89349f-33f1-4152-e34d-bd25e50b3148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder의 각 output (hidden_state)을 담을 배열 생성\n",
        "encoder_outputs = torch.zeros(max_len, hidden_size)\n",
        "encoder_outputs.shape"
      ],
      "metadata": {
        "id": "2T6j-8YZaFMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c3f198-bfbd-4a7f-ef79-984c661a41f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 아래의 그림처럼 hello라는 단어의 중복값을 제거하여 h i e l o 알파뱃으로 언어사전을 만들었다. 이를 각각 원핫인코딩하여 input_size는 5가 된다. 따라서 언어사전 길이가 1000이라고 하면 원핫인코딩하면 한개의 단어를 1000개의 벡터로 표현할 수 있다. \n",
        "\n",
        "![image](https://user-images.githubusercontent.com/101251439/167286530-64ebeee5-1419-44b8-8158-75dbee3b7913.png)"
      ],
      "metadata": {
        "id": "JxGHDWaRmU7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding 층 만들기\n",
        "encoder_embedding = nn.Embedding(input_size, hidden_size)\n",
        "encoder_embedding"
      ],
      "metadata": {
        "id": "KRSJKq-jEZ28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29adecf1-8061-425f-acd9-558578355736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(2506, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN 층 생성\n",
        "encoder_rnn = nn.GRU(hidden_size, hidden_size)\n",
        "encoder_rnn"
      ],
      "metadata": {
        "id": "spil1yeFYhfR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dc349e2-66c2-4b7c-969c-b37b241550b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GRU(256, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 단어만 넣고 임베딩 예시\n",
        "for idx in range(input_length):\n",
        "  input_tensor_step = input_tensor[idx]\n",
        "  print(\"임베딩할 텐서 값 \", input_tensor_step)\n",
        "  embedded = encoder_embedding(input_tensor_step).view(1,1,-1)\n",
        "  # encoder_output, encoder_hidden 생성\n",
        "  encoder_output, encoder_hidden = encoder_rnn(embedded, encoder_hidden)\n",
        "  encoder_outputs[idx] = encoder_output[0,0]\n",
        "  break\n",
        "print(\"Output: \", encoder_outputs)\n",
        "print(encoder_outputs.shape)\n",
        "print(\"Hidden: \", encoder_hidden)\n",
        "print(encoder_hidden.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OId7LZLF_4j",
        "outputId": "fd8ed547-fed2-4891-d299-e59219763cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임베딩할 텐서 값  tensor(2)\n",
            "Output:  tensor([[-0.1800,  0.1494,  0.0140,  ...,  0.4770,  0.0010, -0.0221],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
            "       grad_fn=<CopySlices>)\n",
            "torch.Size([10, 256])\n",
            "Hidden:  tensor([[[-0.1800,  0.1494,  0.0140,  0.2764,  0.1389, -0.0278, -0.3010,\n",
            "           0.1817,  0.2420,  0.1900, -0.0707, -0.3067, -0.0982,  0.2321,\n",
            "          -0.2731, -0.0454, -0.1016,  0.3294, -0.5924,  0.2126, -0.1507,\n",
            "           0.3126,  0.2100, -0.1110, -0.0823, -0.3029,  0.1468, -0.0797,\n",
            "           0.0947,  0.0559,  0.0426, -0.0602,  0.0185,  0.0092, -0.4035,\n",
            "          -0.2461, -0.0476, -0.2472,  0.0834, -0.2411,  0.2606, -0.0691,\n",
            "           0.2050, -0.2181, -0.0790,  0.0682,  0.1005, -0.1808, -0.1647,\n",
            "           0.1259, -0.0410, -0.1839,  0.0115,  0.3885, -0.2289, -0.2041,\n",
            "           0.1733, -0.0084,  0.2846, -0.0184,  0.3019, -0.2970,  0.5535,\n",
            "           0.0980, -0.3433, -0.1224, -0.2115,  0.1178,  0.0301, -0.0668,\n",
            "          -0.0397, -0.1132, -0.1494,  0.0535, -0.2167,  0.2412, -0.2081,\n",
            "           0.4955, -0.0270, -0.1445, -0.1853,  0.2103,  0.3388, -0.2968,\n",
            "          -0.1330,  0.0768,  0.4989, -0.0484, -0.4029, -0.1269,  0.0564,\n",
            "          -0.3243,  0.0047,  0.1290,  0.1776, -0.0332, -0.0513, -0.0218,\n",
            "           0.0267, -0.1571, -0.1792, -0.3675,  0.2644,  0.0772,  0.3557,\n",
            "          -0.3299, -0.0054, -0.1035,  0.2775, -0.1968,  0.0759, -0.1255,\n",
            "          -0.0128,  0.1889, -0.2033, -0.2439,  0.2388, -0.0286, -0.1213,\n",
            "           0.0048,  0.0101,  0.0885,  0.0123,  0.2871,  0.0750, -0.1456,\n",
            "          -0.1881,  0.1586,  0.0107, -0.3479, -0.2465,  0.2774, -0.0259,\n",
            "           0.0391, -0.1211, -0.1808,  0.0082, -0.0587,  0.2629,  0.3814,\n",
            "          -0.3298,  0.1043,  0.4248,  0.2067, -0.1668, -0.2046,  0.2080,\n",
            "          -0.0942, -0.0143,  0.0637,  0.2092,  0.2351,  0.2753, -0.0332,\n",
            "           0.0613,  0.0460, -0.4249,  0.1079,  0.2334, -0.0800, -0.1338,\n",
            "           0.2255,  0.4444,  0.4346, -0.3019,  0.0246,  0.0762, -0.3047,\n",
            "           0.3343, -0.2363,  0.2500, -0.3887, -0.2618, -0.1272, -0.2284,\n",
            "          -0.4046, -0.0688,  0.2193, -0.2411, -0.1717, -0.3240,  0.3301,\n",
            "          -0.2572, -0.0570,  0.2929,  0.2495,  0.1236, -0.0562, -0.0475,\n",
            "           0.1131,  0.1302, -0.3648,  0.1296,  0.0016,  0.0122, -0.0774,\n",
            "          -0.3178, -0.3019,  0.2626, -0.2439, -0.0958, -0.1098,  0.0509,\n",
            "           0.3817,  0.1840,  0.5097,  0.0362,  0.1002,  0.1003,  0.0651,\n",
            "          -0.0841,  0.1421,  0.2908,  0.3355, -0.0501, -0.2670, -0.1376,\n",
            "          -0.0587,  0.0709,  0.3226,  0.3045, -0.1817,  0.1334, -0.4986,\n",
            "          -0.2215, -0.1631, -0.0599,  0.4069,  0.2982,  0.1351, -0.0868,\n",
            "           0.2630, -0.1269, -0.0319, -0.3779,  0.0871,  0.2299,  0.3386,\n",
            "          -0.1205, -0.0677, -0.1339,  0.0583,  0.0488,  0.3315, -0.4491,\n",
            "           0.1979, -0.1955,  0.5656,  0.4001, -0.0297,  0.3504,  0.1136,\n",
            "          -0.2812,  0.4770,  0.0010, -0.0221]]], grad_fn=<StackBackward0>)\n",
            "torch.Size([1, 1, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "encoder를 지난 최종 output (context vector)은 디코더의 첫번째 입력되는 hidden state이다.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/101251439/167286811-38f4696b-6f06-4fba-866b-1abf9bd36af5.png)"
      ],
      "metadata": {
        "id": "FrTZPOZqojfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 전체 사용\n",
        "for idx in range(input_length):\n",
        "  input_tensor_step = input_tensor[idx]\n",
        "  print(\"임베딩할 텐서 값 \", input_tensor_step)\n",
        "  embedded = encoder_embedding(input_tensor_step).view(1,1,-1)\n",
        "  # encoder_output, encoder_hidden 생성\n",
        "  encoder_output, encoder_hidden = encoder_rnn(embedded, encoder_hidden)\n",
        "  encoder_outputs[idx] = encoder_output[0,0]\n",
        "  \n",
        "print(\"Output: \", encoder_outputs)\n",
        "print(encoder_outputs.shape)\n",
        "print(\"Hidden: \", encoder_hidden)\n",
        "print(encoder_hidden.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLLfndHRR-Ee",
        "outputId": "9872ea46-6df2-4815-d3a3-c47f65552db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임베딩할 텐서 값  tensor(2)\n",
            "임베딩할 텐서 값  tensor(3)\n",
            "임베딩할 텐서 값  tensor(4)\n",
            "임베딩할 텐서 값  tensor(5)\n",
            "임베딩할 텐서 값  tensor(6)\n",
            "임베딩할 텐서 값  tensor(5)\n",
            "임베딩할 텐서 값  tensor(7)\n",
            "임베딩할 텐서 값  tensor(8)\n",
            "임베딩할 텐서 값  tensor(9)\n",
            "임베딩할 텐서 값  tensor(1)\n",
            "Output:  tensor([[-0.2101,  0.1963,  0.0205,  ...,  0.6142,  0.0037, -0.0304],\n",
            "        [-0.3297,  0.0476,  0.2311,  ...,  0.2259, -0.0043,  0.4193],\n",
            "        [-0.4934, -0.4131, -0.1565,  ..., -0.3029,  0.4336,  0.4013],\n",
            "        ...,\n",
            "        [-0.0591,  0.5687, -0.3574,  ...,  0.0246,  0.1674, -0.5449],\n",
            "        [ 0.1991, -0.0494, -0.3080,  ..., -0.4123,  0.5717, -0.5242],\n",
            "        [ 0.1855, -0.3274, -0.1775,  ..., -0.4647, -0.0044, -0.4288]],\n",
            "       grad_fn=<CopySlices>)\n",
            "torch.Size([10, 256])\n",
            "Hidden:  tensor([[[ 1.8553e-01, -3.2737e-01, -1.7746e-01,  2.9361e-01, -2.7654e-01,\n",
            "          -1.6257e-02, -9.3238e-02, -2.9453e-01, -3.6550e-01,  5.1483e-01,\n",
            "          -4.0575e-01, -2.0769e-01, -1.6290e-01,  2.2070e-02, -2.6099e-01,\n",
            "          -2.4117e-01, -3.5822e-01,  4.1358e-02, -1.2331e-01,  4.3622e-01,\n",
            "           5.7500e-02, -3.6290e-01, -3.2169e-01,  2.4425e-01, -4.3693e-02,\n",
            "          -1.2849e-01,  3.4084e-01,  3.0567e-01,  7.5472e-02,  7.5786e-02,\n",
            "          -1.7097e-02, -2.7780e-02, -1.0886e-01,  7.9470e-02,  7.5075e-02,\n",
            "          -4.5910e-01,  3.1378e-02,  3.5593e-01, -8.4244e-02, -8.6812e-02,\n",
            "          -1.5236e-01, -1.3729e-01, -1.2754e-01,  1.9763e-01,  1.7052e-02,\n",
            "           3.7005e-01,  1.2101e-01,  1.6412e-01, -2.0437e-01, -2.6039e-01,\n",
            "          -4.8333e-01,  2.3079e-01,  1.1424e-02,  1.4952e-01, -7.5544e-02,\n",
            "          -4.1887e-01,  2.0382e-01, -3.5353e-01,  1.1993e-01,  3.8428e-01,\n",
            "           6.6012e-02, -2.4601e-01,  1.0371e-01,  1.9052e-02, -6.6556e-02,\n",
            "          -4.0570e-01,  9.0233e-02, -7.5007e-02,  3.6542e-01, -2.5494e-01,\n",
            "           5.0446e-01,  4.4830e-01, -1.7160e-01,  2.0022e-01,  5.0578e-01,\n",
            "          -8.6898e-02,  2.6248e-01,  5.7014e-01, -3.3528e-01, -2.7163e-01,\n",
            "           1.4251e-01,  2.4656e-01,  2.0597e-01, -3.8794e-01,  2.1079e-01,\n",
            "          -1.3061e-02,  2.8287e-01, -1.1618e-01,  8.0891e-02,  3.1247e-03,\n",
            "          -1.6827e-01, -2.1751e-02,  2.0819e-01, -2.7487e-01, -9.4698e-02,\n",
            "           5.1001e-02, -1.3804e-01,  2.1864e-01, -8.7541e-02,  9.2098e-02,\n",
            "           1.5366e-01,  1.2446e-01, -3.1948e-01, -7.9883e-02, -2.5002e-01,\n",
            "           1.7433e-01,  3.7732e-01, -5.4707e-01,  3.6940e-02, -2.1193e-01,\n",
            "          -3.6361e-01,  2.1733e-02, -2.7215e-01, -8.4042e-03, -5.0374e-01,\n",
            "           5.8897e-01, -8.9980e-02, -3.5412e-01,  3.4306e-01,  7.2969e-02,\n",
            "           3.9981e-01, -1.9689e-01, -3.6813e-01, -3.0297e-01, -2.4809e-01,\n",
            "          -1.8404e-01, -3.5193e-01, -2.4280e-01,  5.2934e-01,  2.0030e-01,\n",
            "           5.1825e-01,  7.3710e-02, -1.6934e-01,  3.9468e-01,  3.3090e-01,\n",
            "           1.0285e-01,  1.0116e-01,  4.8757e-02,  7.2215e-02, -2.5305e-01,\n",
            "           3.2151e-01,  1.5773e-01,  7.0490e-02,  3.7154e-02, -2.2654e-01,\n",
            "           1.9841e-01, -1.5935e-01, -5.9520e-03,  2.6613e-01,  1.0813e-01,\n",
            "          -2.3759e-01, -1.1725e-01,  1.0883e-01,  5.0219e-01, -1.0235e-01,\n",
            "          -1.5335e-02, -4.0385e-02, -5.1763e-03,  2.5441e-01,  3.4116e-01,\n",
            "           2.0091e-01,  3.5100e-01, -7.1971e-01, -4.6089e-01, -3.6717e-01,\n",
            "          -4.0999e-02,  3.8707e-01, -2.5014e-01,  1.3851e-01,  5.2093e-01,\n",
            "          -2.6153e-01, -3.0655e-01, -3.4004e-01, -2.6730e-01,  1.9310e-01,\n",
            "          -3.7675e-02, -1.5948e-01,  3.9838e-01, -6.4142e-02,  1.8905e-01,\n",
            "          -3.0410e-02, -3.4934e-01, -2.1973e-01,  1.3757e-01, -5.5193e-01,\n",
            "           3.0144e-01, -1.5491e-01,  3.9494e-01, -1.9529e-01,  8.4889e-02,\n",
            "          -2.4529e-01, -1.1893e-01,  6.2899e-02,  4.7063e-01, -3.3886e-01,\n",
            "          -4.0528e-01,  7.6353e-02, -1.9737e-01,  3.5344e-01, -9.3965e-02,\n",
            "          -2.2042e-01, -2.5712e-01, -4.1075e-01,  1.0953e-01,  1.7969e-01,\n",
            "          -5.0786e-02,  3.3239e-04,  3.8911e-01, -2.4529e-01, -1.0148e-01,\n",
            "          -4.6527e-02,  5.1050e-01,  1.1269e-02,  5.1544e-01,  2.5713e-01,\n",
            "           1.4458e-01,  1.4033e-01, -2.0526e-01,  5.3335e-01,  2.2515e-01,\n",
            "           3.1177e-01,  5.5605e-01, -5.4880e-01, -4.4740e-02,  1.1625e-01,\n",
            "          -4.0010e-01,  4.5902e-01, -1.5089e-02,  1.4426e-01, -4.0610e-01,\n",
            "           1.0105e-01,  3.6140e-01,  4.3310e-01,  8.4004e-02, -6.2668e-01,\n",
            "          -1.6764e-03, -1.0335e-03,  6.7559e-02, -2.3461e-01, -2.0065e-01,\n",
            "          -4.5580e-01, -1.6117e-02,  6.5196e-02, -2.4422e-01, -2.0991e-01,\n",
            "          -1.2629e-01, -3.1711e-01, -1.6199e-01,  1.9908e-01,  2.7948e-01,\n",
            "          -2.5601e-03,  4.6142e-01,  1.8507e-01, -4.6472e-01, -4.4442e-03,\n",
            "          -4.2875e-01]]], grad_fn=<StackBackward0>)\n",
            "torch.Size([1, 1, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Decoder\n",
        "\n",
        "디코더 과정에서 인코더의 hidden state를 이용해서 Attention 기법을 사용합니다.\n",
        "Attention 층을 거친 최종 output은 디코더 입력값과 연산을 통해 다음 단어를 예측합니다.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/101251439/167294177-0089f034-8ca5-4185-80d4-4f7ecbc75e28.png)\n",
        "\n",
        "* 디코더에 필요한 입력값을 만듭니다.\n",
        "  * input_size & output_size: decoder의 입력 차원 또는 decoder의 출력 차원 (한국어 언어 사전 길이에 해당된다.) \n",
        "\n"
      ],
      "metadata": {
        "id": "FKrGN6GppAgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kor_dic_size = kor_dic.n_words\n",
        "output_size = kor_dic_size\n",
        "hidden_size = 256\n",
        "print(\"decoder_hiddenstate: \", hidden_size)\n",
        "print(\"decoder_input_size or output_size: \", output_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Eo4CaMD-xBp",
        "outputId": "e2ba6b6b-ceae-48e4-fe56-72359795829a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoder_hiddenstate:  256\n",
            "decoder_input_size or output_size:  3241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * Decoder 동작 방식 : SOS 토큰 입력 -> 임배딩 -> dropout -> Attention -> RNN -> output\n",
        "\n",
        "  1. Decoder 첫 입력값 SOS 토큰 백터 만들기\n",
        "  2. 디코더 임배딩, 드롭아웃 층 생성\n",
        "  3. 디코더 RNN 층 생성\n",
        "  4. 어텐션 층 생성\n",
        "  \n"
      ],
      "metadata": {
        "id": "CAGqIEGI5Ohw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 디코더에 필요한 층 생성"
      ],
      "metadata": {
        "id": "hZW_Gbozs2qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#decoder_embedding\n",
        "decoder_embedding = nn.Embedding(output_size, hidden_size)\n",
        "decoder_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwPYB0KuWoh_",
        "outputId": "a55f30d9-1089-4916-f0b8-c611c09ac597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(3241, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dropout layer\n",
        "dropout = nn.Dropout(0.1)"
      ],
      "metadata": {
        "id": "yf_mpMKz6In6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rnn layer 생성\n",
        "decoder_rnn = nn.GRU(hidden_size, hidden_size)\n",
        "decoder_rnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SYHBbDuWIx5",
        "outputId": "1893fc5c-8889-4576-8369-66078c99e47b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GRU(256, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 디코더의 다음 단어를 생성하기 위해 인코더에 입력된 단어 중 어느 단어에 집중을 할까? 집중해야할 단어들에 점수(Attention Score)를 부여하기 위해 위한 어텐션 층을 생성한다."
      ],
      "metadata": {
        "id": "LiL16S0B1tCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#attention layer: 512 길이를 10으로 축소한다.\n",
        "attn = nn.Linear(hidden_size * 2, max_len)\n",
        "attn"
      ],
      "metadata": {
        "id": "LV-aEfWoFhIr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c16fd9-f248-4633-90e8-0ee1d4671105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=512, out_features=10, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#attention combine\n",
        "attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
        "attn_combine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc9A2Cl8rBHw",
        "outputId": "565a0bdd-f7d7-4ed8-ade3-014e25042deb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=512, out_features=256, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = nn.Linear(hidden_size, output_size)"
      ],
      "metadata": {
        "id": "bAa1oa_J7Twf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 디코더 첫번째 SOS 토큰 입력값 만들기"
      ],
      "metadata": {
        "id": "m-vWnajrtFC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input = torch.tensor([[SOS_token]])\n",
        "decoder_input"
      ],
      "metadata": {
        "id": "XxXFpCrtV7EO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b48c2c5-8346-45cf-aaef-7812b0880b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* decoder_hidden : 첫번째 hiddens state는 인코더의 최종 ouput"
      ],
      "metadata": {
        "id": "UGnJYsSSegzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder에 처음 입력되는 hidden은 encoder에서 출력된 context vector이다.\n",
        "decoder_hidden = encoder_hidden\n",
        "decoder_hidden"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AK7rSFn5lsz",
        "outputId": "cd875a4f-759e-4097-ce43-2557d9daea15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.8553e-01, -3.2737e-01, -1.7746e-01,  2.9361e-01, -2.7654e-01,\n",
              "          -1.6257e-02, -9.3238e-02, -2.9453e-01, -3.6550e-01,  5.1483e-01,\n",
              "          -4.0575e-01, -2.0769e-01, -1.6290e-01,  2.2070e-02, -2.6099e-01,\n",
              "          -2.4117e-01, -3.5822e-01,  4.1358e-02, -1.2331e-01,  4.3622e-01,\n",
              "           5.7500e-02, -3.6290e-01, -3.2169e-01,  2.4425e-01, -4.3693e-02,\n",
              "          -1.2849e-01,  3.4084e-01,  3.0567e-01,  7.5472e-02,  7.5786e-02,\n",
              "          -1.7097e-02, -2.7780e-02, -1.0886e-01,  7.9470e-02,  7.5075e-02,\n",
              "          -4.5910e-01,  3.1378e-02,  3.5593e-01, -8.4244e-02, -8.6812e-02,\n",
              "          -1.5236e-01, -1.3729e-01, -1.2754e-01,  1.9763e-01,  1.7052e-02,\n",
              "           3.7005e-01,  1.2101e-01,  1.6412e-01, -2.0437e-01, -2.6039e-01,\n",
              "          -4.8333e-01,  2.3079e-01,  1.1424e-02,  1.4952e-01, -7.5544e-02,\n",
              "          -4.1887e-01,  2.0382e-01, -3.5353e-01,  1.1993e-01,  3.8428e-01,\n",
              "           6.6012e-02, -2.4601e-01,  1.0371e-01,  1.9052e-02, -6.6556e-02,\n",
              "          -4.0570e-01,  9.0233e-02, -7.5007e-02,  3.6542e-01, -2.5494e-01,\n",
              "           5.0446e-01,  4.4830e-01, -1.7160e-01,  2.0022e-01,  5.0578e-01,\n",
              "          -8.6898e-02,  2.6248e-01,  5.7014e-01, -3.3528e-01, -2.7163e-01,\n",
              "           1.4251e-01,  2.4656e-01,  2.0597e-01, -3.8794e-01,  2.1079e-01,\n",
              "          -1.3061e-02,  2.8287e-01, -1.1618e-01,  8.0891e-02,  3.1247e-03,\n",
              "          -1.6827e-01, -2.1751e-02,  2.0819e-01, -2.7487e-01, -9.4698e-02,\n",
              "           5.1001e-02, -1.3804e-01,  2.1864e-01, -8.7541e-02,  9.2098e-02,\n",
              "           1.5366e-01,  1.2446e-01, -3.1948e-01, -7.9883e-02, -2.5002e-01,\n",
              "           1.7433e-01,  3.7732e-01, -5.4707e-01,  3.6940e-02, -2.1193e-01,\n",
              "          -3.6361e-01,  2.1733e-02, -2.7215e-01, -8.4042e-03, -5.0374e-01,\n",
              "           5.8897e-01, -8.9980e-02, -3.5412e-01,  3.4306e-01,  7.2969e-02,\n",
              "           3.9981e-01, -1.9689e-01, -3.6813e-01, -3.0297e-01, -2.4809e-01,\n",
              "          -1.8404e-01, -3.5193e-01, -2.4280e-01,  5.2934e-01,  2.0030e-01,\n",
              "           5.1825e-01,  7.3710e-02, -1.6934e-01,  3.9468e-01,  3.3090e-01,\n",
              "           1.0285e-01,  1.0116e-01,  4.8757e-02,  7.2215e-02, -2.5305e-01,\n",
              "           3.2151e-01,  1.5773e-01,  7.0490e-02,  3.7154e-02, -2.2654e-01,\n",
              "           1.9841e-01, -1.5935e-01, -5.9520e-03,  2.6613e-01,  1.0813e-01,\n",
              "          -2.3759e-01, -1.1725e-01,  1.0883e-01,  5.0219e-01, -1.0235e-01,\n",
              "          -1.5335e-02, -4.0385e-02, -5.1763e-03,  2.5441e-01,  3.4116e-01,\n",
              "           2.0091e-01,  3.5100e-01, -7.1971e-01, -4.6089e-01, -3.6717e-01,\n",
              "          -4.0999e-02,  3.8707e-01, -2.5014e-01,  1.3851e-01,  5.2093e-01,\n",
              "          -2.6153e-01, -3.0655e-01, -3.4004e-01, -2.6730e-01,  1.9310e-01,\n",
              "          -3.7675e-02, -1.5948e-01,  3.9838e-01, -6.4142e-02,  1.8905e-01,\n",
              "          -3.0410e-02, -3.4934e-01, -2.1973e-01,  1.3757e-01, -5.5193e-01,\n",
              "           3.0144e-01, -1.5491e-01,  3.9494e-01, -1.9529e-01,  8.4889e-02,\n",
              "          -2.4529e-01, -1.1893e-01,  6.2899e-02,  4.7063e-01, -3.3886e-01,\n",
              "          -4.0528e-01,  7.6353e-02, -1.9737e-01,  3.5344e-01, -9.3965e-02,\n",
              "          -2.2042e-01, -2.5712e-01, -4.1075e-01,  1.0953e-01,  1.7969e-01,\n",
              "          -5.0786e-02,  3.3239e-04,  3.8911e-01, -2.4529e-01, -1.0148e-01,\n",
              "          -4.6527e-02,  5.1050e-01,  1.1269e-02,  5.1544e-01,  2.5713e-01,\n",
              "           1.4458e-01,  1.4033e-01, -2.0526e-01,  5.3335e-01,  2.2515e-01,\n",
              "           3.1177e-01,  5.5605e-01, -5.4880e-01, -4.4740e-02,  1.1625e-01,\n",
              "          -4.0010e-01,  4.5902e-01, -1.5089e-02,  1.4426e-01, -4.0610e-01,\n",
              "           1.0105e-01,  3.6140e-01,  4.3310e-01,  8.4004e-02, -6.2668e-01,\n",
              "          -1.6764e-03, -1.0335e-03,  6.7559e-02, -2.3461e-01, -2.0065e-01,\n",
              "          -4.5580e-01, -1.6117e-02,  6.5196e-02, -2.4422e-01, -2.0991e-01,\n",
              "          -1.2629e-01, -3.1711e-01, -1.6199e-01,  1.9908e-01,  2.7948e-01,\n",
              "          -2.5601e-03,  4.6142e-01,  1.8507e-01, -4.6472e-01, -4.4442e-03,\n",
              "          -4.2875e-01]]], grad_fn=<StackBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN의 첫 입력값인 SOS 토큰을 임베딩 후 dropout을 적용한다.\n",
        "decoder_embedded = decoder_embedding(decoder_input).view(1,1,-1)\n",
        "decoder_embedded = dropout(decoder_embedded)\n",
        "decoder_embedded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mI4zEcBVoX8",
        "outputId": "9c10925c-b6f4-43df-c32c-7760acf52f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 8.6903e-01,  1.7354e+00,  4.2700e-01, -1.1472e+00, -2.4283e+00,\n",
              "           9.0323e-02,  1.7649e+00, -4.3337e-02, -2.2300e+00, -5.9895e-01,\n",
              "          -1.2797e+00, -1.3895e+00, -5.8978e-01,  1.1077e+00, -1.0980e-01,\n",
              "          -2.7446e+00,  9.1430e-01,  6.2423e-01,  2.4137e+00, -2.2907e-01,\n",
              "          -6.8633e-02,  2.5096e-02,  1.0036e+00,  0.0000e+00,  5.6463e-01,\n",
              "          -1.0815e+00, -2.2506e+00, -7.6832e-02,  2.7149e-01,  7.5214e-01,\n",
              "          -2.3974e-01, -2.4504e-01,  2.8858e+00,  4.2053e-01, -3.7766e-01,\n",
              "          -6.8074e-01, -1.3725e-01,  4.3047e-01, -1.3843e+00,  2.4304e+00,\n",
              "           0.0000e+00,  3.4347e-02,  2.7435e-01, -0.0000e+00, -0.0000e+00,\n",
              "          -2.9426e-02, -1.8375e-01,  5.7472e-01,  6.1659e-01, -1.3018e+00,\n",
              "          -3.7993e-01, -1.8310e+00,  3.7711e-02, -0.0000e+00,  0.0000e+00,\n",
              "           1.2380e+00,  2.8910e+00, -9.2709e-02,  1.6688e-01, -6.2623e-01,\n",
              "           7.0644e-01,  4.0571e-01,  6.2116e-01, -1.6380e-01, -1.9447e-01,\n",
              "           1.5601e-01, -0.0000e+00, -4.3129e-01,  2.7681e+00,  7.0898e-04,\n",
              "          -3.9320e-01,  4.3516e-01, -2.1400e+00, -1.5438e+00, -0.0000e+00,\n",
              "           4.4676e-01,  1.1101e+00, -1.0191e+00,  4.7763e-01,  2.5961e+00,\n",
              "          -4.8456e-01,  1.1323e+00,  3.8507e-01,  1.5656e+00, -1.4805e+00,\n",
              "          -1.1995e+00, -9.7418e-01,  0.0000e+00,  7.1803e-01,  0.0000e+00,\n",
              "          -9.0441e-01,  1.2495e+00, -1.0358e+00,  1.0889e+00,  1.8716e+00,\n",
              "          -7.7931e-01,  7.3851e-01, -9.4630e-01,  1.2255e+00, -1.9631e+00,\n",
              "           1.3799e-01, -6.8307e-01, -1.4733e+00, -8.9116e-01,  1.7283e+00,\n",
              "          -2.8519e+00,  1.7900e+00, -1.6264e+00,  4.4136e-01,  5.5862e-01,\n",
              "           7.5769e-01, -0.0000e+00,  4.7158e-01,  0.0000e+00,  7.2322e-01,\n",
              "          -3.9041e-01, -1.0412e+00, -9.9369e-01, -6.1639e-01, -7.8190e-01,\n",
              "           7.9609e-01, -0.0000e+00, -8.8028e-01, -0.0000e+00,  3.3486e-01,\n",
              "          -0.0000e+00, -1.0692e+00,  3.3625e-02,  1.1537e+00,  1.6437e+00,\n",
              "          -9.4654e-01,  4.7388e-01, -8.1023e-02, -1.3475e+00,  2.2122e-01,\n",
              "           3.9348e-01, -7.2860e-01,  1.4318e-01, -2.7751e-01, -1.3873e+00,\n",
              "           2.2904e+00,  5.9824e-02, -2.5415e+00, -1.0277e+00, -3.5788e-01,\n",
              "           2.7489e-01,  5.8688e-01, -1.6669e+00, -1.6840e+00,  3.0936e-01,\n",
              "          -1.5638e+00,  6.2088e-01,  8.9807e-01, -7.8498e-01, -1.0874e+00,\n",
              "           1.0929e-02, -7.1622e-01,  0.0000e+00,  5.5568e-01, -3.0230e+00,\n",
              "           4.6427e-01, -1.6550e+00, -3.5426e-01,  5.8200e-01, -4.0111e-01,\n",
              "          -1.5226e+00, -6.6196e-01,  1.1026e+00,  2.0984e+00, -0.0000e+00,\n",
              "           6.7102e-01, -5.7540e-01, -2.2602e-01, -1.0143e+00,  0.0000e+00,\n",
              "           4.5598e-01,  2.6474e-01, -1.3059e+00, -0.0000e+00,  7.9607e-01,\n",
              "          -1.1158e+00,  6.1803e-01,  0.0000e+00,  0.0000e+00, -2.9984e-01,\n",
              "           1.4853e-01, -1.1153e+00, -3.3825e-01,  2.3594e-01, -3.4971e+00,\n",
              "           9.8115e-01,  0.0000e+00, -1.3827e+00,  3.5527e-01, -1.2069e+00,\n",
              "          -1.3649e+00,  3.5369e-01, -0.0000e+00,  6.3809e-01, -2.7258e-01,\n",
              "           8.9946e-01,  1.1100e+00,  2.7813e-01,  2.2703e-01, -8.6998e-01,\n",
              "           3.3595e-01,  0.0000e+00, -6.9573e-01, -1.4399e+00, -1.7679e+00,\n",
              "           6.4797e-01,  1.5929e-01, -2.9809e-01, -8.0552e-01, -1.0342e+00,\n",
              "          -4.3371e-01,  7.7511e-01, -3.7862e-01,  6.0024e-01,  2.2931e+00,\n",
              "          -9.6925e-01, -1.0829e+00,  1.5989e+00,  1.6653e+00,  4.2369e-01,\n",
              "          -7.4755e-01,  1.6618e-01,  6.2926e-02,  2.3212e+00,  2.6362e-01,\n",
              "           4.6440e-01,  5.4052e-01,  1.7119e+00, -7.7613e-01,  2.1085e+00,\n",
              "           1.4123e+00,  4.1349e-02,  8.4646e-01,  3.0114e-01, -7.3356e-01,\n",
              "           0.0000e+00,  1.1139e-01,  2.8350e+00, -1.1241e+00,  9.7369e-01,\n",
              "           0.0000e+00, -1.6071e+00,  1.5802e+00,  4.2332e-01,  1.6350e-01,\n",
              "          -0.0000e+00, -0.0000e+00,  2.3922e+00, -2.5010e-01,  0.0000e+00,\n",
              "          -0.0000e+00]]], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 어텐션 \n",
        "디코더 임베딩(256) 값과 인코더의 최종 output(256)을 결합 후 attention layer(512)에 입력한다. 인코더의 최종 output은 전체 문장의 문맥 정보를 갖고 있다. 디코더 입력과의 연산을 통해 다음 단어 출력시 인코더의 어느 부분에 집중할 지에 대해 고려할 수 있다. softmax를 거쳐 어디에 집중을 할지에 대한 비율이 출력된다.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/101251439/167294783-c0428b93-485d-418b-b1b1-76499e4ae3c8.png)"
      ],
      "metadata": {
        "id": "IpIj0QxbaCkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_attention = attn(torch.cat((decoder_embedded[0], decoder_hidden[0]), 1))\n",
        "decoder_attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5-A4eDpXLPt",
        "outputId": "bd9ed620-f105-459a-d77e-82236e79bcb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.3025,  0.5066,  0.0562, -0.1150,  0.4674,  0.2874,  0.5108,  0.0790,\n",
              "         -0.8283,  0.2255]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weight = F.softmax(decoder_attention, dim=1)\n",
        "attn_weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7tTAlibYAo4",
        "outputId": "7473f770-d866-4191-d0b2-f7b33a1b66f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0631, 0.1417, 0.0903, 0.0761, 0.1362, 0.1138, 0.1423, 0.0924, 0.0373,\n",
              "         0.1070]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "인코더의 각 output에 어텐션을 적용하여 어느 output에 집중할지 계산한다. 예를 들어 \"I am a student\" 가 있을 때 encoder의 총 output은 4개를 갖게된다. 각 output은 다음과 같은 정보를 갖고 있다. \n",
        "\n",
        "1. 첫번째 output: I\n",
        "2. 두번째 output: I am\n",
        "3. 세번째 output: I am a \n",
        "4. 네번째 output: I am a student\n",
        "\n",
        "4개의 output 중 어디에 집중을 할지는 attention wight을 곱하여 가장 높은 점수의 output에 집중하게 된다. 인코더의 각 output에 어텐션을 적용하여 attn_applied를 구한다."
      ],
      "metadata": {
        "id": "jlVyEySV4QAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_applied = torch.bmm(attn_weight.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "attn_applied"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY9rJn_7VQib",
        "outputId": "fe03b8fa-2072-4c24-dc1f-20b78366e7a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.2038,  0.2284, -0.0172,  0.0805, -0.0537,  0.0781, -0.0471,\n",
              "          -0.0059,  0.1504,  0.1661, -0.0348, -0.1466,  0.0420,  0.0879,\n",
              "           0.0175, -0.0273, -0.3623,  0.1270, -0.2731,  0.0968, -0.0994,\n",
              "          -0.1236, -0.1177, -0.1458, -0.2581, -0.0099,  0.0826, -0.1984,\n",
              "           0.3341,  0.1162,  0.1301, -0.1599, -0.2528, -0.0453, -0.2783,\n",
              "          -0.1302,  0.0562,  0.0354,  0.2389, -0.1730,  0.0317, -0.0515,\n",
              "           0.2313, -0.0363, -0.1642,  0.1484,  0.0613,  0.0418,  0.1846,\n",
              "          -0.0753, -0.3043,  0.0563, -0.0286, -0.1338, -0.0197, -0.0507,\n",
              "           0.1117, -0.1336,  0.3326,  0.0676,  0.1393, -0.1731,  0.4065,\n",
              "           0.0574, -0.0368, -0.0194,  0.2230,  0.1812,  0.1362,  0.1259,\n",
              "           0.0841, -0.0642, -0.1908,  0.1496,  0.1728,  0.1307, -0.0377,\n",
              "           0.4715, -0.0162, -0.3498, -0.1698,  0.2358,  0.2808, -0.2156,\n",
              "          -0.0009, -0.1918,  0.1833,  0.0839, -0.3719, -0.0013, -0.2278,\n",
              "          -0.0509,  0.0698, -0.1500, -0.0149,  0.0175,  0.1271,  0.2273,\n",
              "          -0.0905, -0.2624, -0.1503, -0.1259,  0.3681,  0.0981, -0.0253,\n",
              "          -0.2740, -0.0055, -0.1395,  0.1669,  0.1522,  0.0758,  0.1330,\n",
              "          -0.0187, -0.0206, -0.0927, -0.0605,  0.1763,  0.1752,  0.2347,\n",
              "           0.0574,  0.1097, -0.0781,  0.1725,  0.0072, -0.1977, -0.3045,\n",
              "          -0.0278,  0.3458, -0.1138, -0.1429, -0.0185,  0.0031,  0.0894,\n",
              "           0.2882,  0.0474, -0.1138, -0.1349, -0.1168,  0.2598, -0.0974,\n",
              "          -0.0284, -0.0250,  0.1593,  0.2192, -0.1381, -0.2468, -0.0767,\n",
              "          -0.0038, -0.0331,  0.0273,  0.2581,  0.1931,  0.2568,  0.1457,\n",
              "          -0.1523, -0.1774, -0.3284,  0.0354,  0.1406,  0.1299, -0.0507,\n",
              "          -0.0054, -0.0905,  0.0719, -0.3066, -0.0129,  0.2426, -0.3425,\n",
              "          -0.0630,  0.2971, -0.0883, -0.3389, -0.0197, -0.0467,  0.0254,\n",
              "          -0.3026,  0.0266,  0.0102, -0.0456, -0.0501, -0.0970,  0.1331,\n",
              "          -0.1064,  0.1520, -0.2676,  0.3695,  0.2640,  0.2470,  0.0827,\n",
              "           0.2054,  0.0486,  0.0902,  0.0371,  0.1176, -0.1026, -0.2608,\n",
              "          -0.2052, -0.0795,  0.0212, -0.0172,  0.2044,  0.0015, -0.1227,\n",
              "           0.0625,  0.1245,  0.4591, -0.2766, -0.1258, -0.0153,  0.0207,\n",
              "          -0.0796,  0.0044, -0.0827,  0.1693, -0.0063,  0.1207, -0.1580,\n",
              "          -0.0871,  0.1082,  0.0504,  0.1654,  0.2896, -0.2404, -0.1619,\n",
              "           0.0090,  0.0384, -0.1145,  0.2226,  0.3508,  0.0675, -0.0116,\n",
              "           0.0704, -0.0910,  0.2113, -0.1904,  0.2551,  0.1134, -0.2424,\n",
              "          -0.1381, -0.0629,  0.0980, -0.1208, -0.0239,  0.0945,  0.0492,\n",
              "           0.1767, -0.1445,  0.1209,  0.0576,  0.3114,  0.1349,  0.1497,\n",
              "          -0.0182,  0.1349,  0.1314, -0.0490]]], grad_fn=<BmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 실제 다음 단어를 예측하기 위해 방금 구한 Attention Value와 디코더의 입력값과 결합한다. "
      ],
      "metadata": {
        "id": "TS8EFUw354Sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = torch.cat((decoder_embedded[0], attn_applied[0]), 1)\n",
        "output = attn_combine(output).unsqueeze(0)\n",
        "output = F.relu(output)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuBisug2ZbIt",
        "outputId": "7c89ee3e-f29e-4211-ccad-757a2d2ed050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[2.2032e-03, 0.0000e+00, 4.5133e-01, 5.5458e-01, 3.6334e-01,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2375e-01, 4.1301e-01,\n",
              "          0.0000e+00, 6.3971e-01, 1.2202e-01, 3.6956e-01, 2.3268e-01,\n",
              "          0.0000e+00, 0.0000e+00, 2.3699e-01, 0.0000e+00, 0.0000e+00,\n",
              "          3.4387e-01, 7.3438e-02, 0.0000e+00, 0.0000e+00, 7.6014e-01,\n",
              "          0.0000e+00, 9.4920e-01, 1.0621e+00, 1.6924e-01, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 5.0330e-01, 2.2878e-01, 4.3727e-03,\n",
              "          0.0000e+00, 7.3372e-01, 6.7497e-01, 5.6222e-01, 3.7617e-01,\n",
              "          0.0000e+00, 3.4560e-01, 2.4533e-01, 9.6301e-01, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6069e-01, 2.1328e-01,\n",
              "          4.2347e-01, 0.0000e+00, 4.2865e-01, 0.0000e+00, 9.5506e-02,\n",
              "          0.0000e+00, 1.9383e-01, 0.0000e+00, 0.0000e+00, 7.9055e-01,\n",
              "          0.0000e+00, 9.3617e-02, 0.0000e+00, 0.0000e+00, 6.9324e-02,\n",
              "          0.0000e+00, 0.0000e+00, 1.4140e-01, 0.0000e+00, 4.9590e-01,\n",
              "          5.9863e-01, 0.0000e+00, 1.5502e-01, 3.2781e-01, 0.0000e+00,\n",
              "          4.9026e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "          6.4148e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5979e-01,\n",
              "          6.9900e-02, 4.1691e-01, 3.8302e-01, 0.0000e+00, 0.0000e+00,\n",
              "          3.9854e-01, 0.0000e+00, 1.1305e-01, 1.0364e-01, 6.2970e-01,\n",
              "          0.0000e+00, 2.6436e-01, 4.6423e-01, 4.3637e-01, 6.5255e-01,\n",
              "          2.4588e-01, 0.0000e+00, 6.6618e-01, 0.0000e+00, 4.3748e-02,\n",
              "          6.2294e-02, 0.0000e+00, 1.2537e+00, 0.0000e+00, 1.4791e-02,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.0487e-02,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3216e-01, 8.9785e-01,\n",
              "          3.4545e-01, 0.0000e+00, 0.0000e+00, 3.2045e-01, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "          0.0000e+00, 2.7895e-01, 0.0000e+00, 3.5575e-02, 2.6953e-01,\n",
              "          1.5309e-01, 4.1285e-01, 6.6752e-01, 3.0899e-01, 0.0000e+00,\n",
              "          3.7239e-01, 9.6790e-02, 0.0000e+00, 1.6163e-01, 1.2508e-01,\n",
              "          9.7138e-02, 1.8640e-01, 0.0000e+00, 0.0000e+00, 7.2775e-01,\n",
              "          0.0000e+00, 1.0880e-01, 0.0000e+00, 0.0000e+00, 9.4188e-01,\n",
              "          2.6571e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "          1.3122e-01, 2.7825e-01, 1.0512e-01, 2.4566e-01, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5884e-01, 3.9299e-01,\n",
              "          1.0597e-01, 0.0000e+00, 1.9388e-01, 4.1671e-01, 2.5886e-01,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "          9.1091e-02, 0.0000e+00, 0.0000e+00, 3.7666e-01, 0.0000e+00,\n",
              "          6.7035e-01, 6.9183e-01, 0.0000e+00, 3.8010e-01, 7.7810e-01,\n",
              "          5.7926e-01, 0.0000e+00, 4.9319e-01, 1.5051e-01, 0.0000e+00,\n",
              "          1.2014e-01, 0.0000e+00, 4.3241e-01, 0.0000e+00, 9.5226e-02,\n",
              "          0.0000e+00, 0.0000e+00, 4.0291e-01, 2.7231e-02, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 3.5666e-01, 4.9648e-02, 5.9716e-02,\n",
              "          1.2518e-01, 2.6577e-01, 0.0000e+00, 7.5693e-02, 0.0000e+00,\n",
              "          5.9567e-01, 1.5982e-04, 7.8046e-02, 0.0000e+00, 5.8818e-01,\n",
              "          0.0000e+00, 1.9825e-01, 0.0000e+00, 0.0000e+00, 3.5915e-01,\n",
              "          7.0037e-02, 0.0000e+00, 1.6820e-01, 0.0000e+00, 2.2542e-01,\n",
              "          9.0805e-01, 3.1986e-01, 6.2399e-01, 3.8038e-01, 0.0000e+00,\n",
              "          3.0586e-01, 4.6532e-01, 1.8863e-01, 0.0000e+00, 0.0000e+00,\n",
              "          2.2158e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "          8.8422e-01, 0.0000e+00, 6.1103e-01, 0.0000e+00, 0.0000e+00,\n",
              "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "          1.7667e-01]]], grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Attention을 거쳐 나온 ouput과 첫번째 sos 토큰의 output을 rnn에 입력한다. 이후 나오는 output은 다음 단어의 백터값을 의미한다.\n",
        "![image](https://user-images.githubusercontent.com/101251439/167295080-328e0b1f-46c6-48fc-a966-44dec191a27e.png)"
      ],
      "metadata": {
        "id": "loip6hAkhMdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output, decoder_hidden = decoder_rnn(output, decoder_hidden)\n",
        "decoder_output = F.log_softmax(out(output[0]), dim=1)\n",
        "_, topi = decoder_output.topk(1)\n",
        "print(topi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHCwrcwl6_tQ",
        "outputId": "fe895000-45da-45a4-82e3-d75c96574779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1912]])\n"
          ]
        }
      ]
    }
  ]
}