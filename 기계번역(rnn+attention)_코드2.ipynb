{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "기계번역(rnn+attention) 코드2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bcmin1018/NLP/blob/main/%EA%B8%B0%EA%B3%84%EB%B2%88%EC%97%AD(rnn%2Battention)_%EC%BD%94%EB%93%9C2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "import nltk\n",
        "import codecs\n",
        "import csv\n",
        "from konlpy.tag import Okt\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1UZWeIdFv6z",
        "outputId": "bce5dee7-b53e-47f4-dde0-2c36350273b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 37.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.2.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZzBwTKoY3Eo",
        "outputId": "1e22db1b-6379-4837-e530-54d71ecf272f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문어체_뉴스 다운로드\n",
        "!gdown --id 1rRvQ9n52tXQgskDz_Q8F12zSlCedp9gG\n",
        "# !gdown --id 1lXi-K8cs9gih6j3FyTKWLtapHh1JmWkD\n",
        "# !gdown --id 1u9DAVk4bNjE8-qiLpUQRloFpjckV_KjO\n",
        "# !gdown --id 1IpxnKROkWiLiJGOK3DoHuzX2oBEjC5ks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3swmxdjCQQ3",
        "outputId": "10cc9dda-bcf6-4066-b6e3-68659feef245"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rRvQ9n52tXQgskDz_Q8F12zSlCedp9gG\n",
            "To: /content/3_문어체_뉴스(1)_200226.xlsx\n",
            "100% 51.4M/51.4M [00:00<00:00, 134MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문어체 뉴스 데이터의 파일 정보\n",
        "train_data_path= \"./3_문어체_뉴스(1)_200226.xlsx\""
      ],
      "metadata": {
        "id": "D3PI9BZQDSIo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data = pd.read_excel(train_data_path)\n",
        "# train_data = train_data[['원문', '번역문']].values\n",
        "train_data = train_data[['원문', '번역문']].values[0:100]"
      ],
      "metadata": {
        "id": "5NESZSR3FZCE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "27645e7e-9837-45af-f4c0-b36f80fef3a6"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-e6f912b832eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train_data = pd.read_excel(train_data_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# train_data = train_data[['원문', '번역문']].values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'원문'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'번역문'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUMT(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, max_length, device, dropout_p=0.1):\n",
        "        super(GRUMT, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        self.device = device\n",
        "\n",
        "        # Encoder 부분\n",
        "        self.encoder_embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.encoder_rnn = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "\n",
        "        # Decoder 부분\n",
        "        self.decoder_rnn = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.decoder_embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "        self.loss = nn.NLLLoss()\n",
        "\n",
        "    def _encoder(self, input_tensor, input_length):\n",
        "        encoder_hidden = self._init_hidden()\n",
        "        encoder_outputs = torch.zeros(self.max_length, self.hidden_size, device=self.device)\n",
        "\n",
        "        for idx in range(input_length):\n",
        "            input_tensor_step = input_tensor[idx]\n",
        "            embedded = self.encoder_embedding(input_tensor_step).view(1, 1, -1)\n",
        "            encoder_output, encoder_hidden = self.encoder_rnn(embedded, encoder_hidden)\n",
        "            encoder_outputs[idx] = encoder_output[0, 0]\n",
        "\n",
        "        return encoder_outputs, encoder_hidden\n",
        "\n",
        "    def _decoder(self, target_tensor, target_length, encoder_hidden, encoder_outputs):\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=self.device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        loss_sum = 0\n",
        "        for di in range(target_length):\n",
        "            embedded = self.decoder_embedding(decoder_input).view(1, 1, -1)\n",
        "            embedded = self.dropout(embedded)\n",
        "\n",
        "            decoder_attention = F.softmax(self.attn(torch.cat((embedded[0], decoder_hidden[0]), 1)), dim=1)\n",
        "            attn_applied = torch.bmm(decoder_attention.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "\n",
        "            output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "            output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "            output = F.relu(output)\n",
        "            output, decoder_hidden = self.decoder_rnn(output, decoder_hidden)\n",
        "\n",
        "            decoder_output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "            target_output = torch.tensor([target_tensor[di]], device=self.device)\n",
        "            loss_sum += self.loss(decoder_output, target_output)\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "        return loss_sum\n",
        "\n",
        "    def forward(self, input_tensor, input_length, target_tensor, target_length):\n",
        "        encoder_outputs, encoder_hidden = self._encoder(input_tensor, input_length)\n",
        "        loss_sum = self._decoder(target_tensor, target_length, encoder_hidden, encoder_outputs)\n",
        "        return loss_sum\n",
        "\n",
        "    def _init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=self.device)"
      ],
      "metadata": {
        "id": "5WjUF77aOtVI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dic(data):\n",
        "  eng_tokenizer = word_tokenize\n",
        "  kor_tokenizer = Okt().morphs\n",
        "\n",
        "  eng_dic = LangDic('en', eng_tokenizer)\n",
        "  kor_dic = LangDic('ko', kor_tokenizer)\n",
        "\n",
        "  for idx in range(0, len(data)):\n",
        "    kor_sen = data[idx][0]\n",
        "    eng_sen = data[idx][1]\n",
        "\n",
        "    kor_dic.add_sentence(eng_sen)\n",
        "    eng_dic.add_sentence(kor_sen)\n",
        "    \n",
        "  return eng_dic, kor_dic\n",
        "\n",
        "def make_data_loader(data_pairs, eng_dic, kor_dic, max_len, batch_size):\n",
        "    ds = MTDataset(data_pairs, eng_dic, kor_dic, max_len)\n",
        "    return DataLoader(ds, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "BAzTxn0jGLKa"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MTDataset(Dataset):\n",
        "    def __init__(self, data_pairs, eng_dic, kor_dic, max_len):\n",
        "        super(MTDataset, self).__init__()\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.pair_data = list()\n",
        "\n",
        "        for idx in range(0, len(data_pairs)):\n",
        "          kor_sen = data_pairs[idx][0]\n",
        "          eng_sen = data_pairs[idx][1]\n",
        "          eng_sen_words, eng_sen_len = eng_dic.sentence2tensor(eng_sen, max_len)\n",
        "          kor_sen_words, kor_sen_len = kor_dic.sentence2tensor(kor_sen, max_len)\n",
        "          self.pair_data.append((eng_sen_words, eng_sen_len, kor_sen_words, kor_sen_len))\n",
        "\n",
        "        self.data_len = len(self.pair_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        eng_sen_words, eng_sen_len, kor_sen_words, kor_sen_len = self.pair_data[idx]\n",
        "        return eng_sen_words, eng_sen_len, kor_sen_words, kor_sen_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_len"
      ],
      "metadata": {
        "id": "DwCWUxPHJ0Jx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LangDic:\n",
        "    # 언어마다 단어 사전을 정의합니다.\n",
        "    def __init__(self, name, tokenizer):\n",
        "        # 클래스의 첫 시작인 함수입니다. 여기서 모델에 필요한 여러 변수들을 정의합니다.\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"UNK\"}\n",
        "        self.n_words = 2\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        # 문장을 받아 문장에서 단어를 확인합니다.\n",
        "        for word in self.tokenizer(sentence):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        # 단어를 보고 그 단어가 사전에 존재하는지 아닌지를 살펴봅니다.\n",
        "        # 존재하지 않는 경우 단어를 사전에 등록합니다.\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def sentence2tensor(self, sentence, max_len):\n",
        "        # 가지고 있는 사전을 바탕으로 문장을 tensor의 형태로 바꿉니다.\n",
        "        # tensor 내 들어있는 값은 단어 index이며 이를 통해 모델의 임베딩에 입력으로 줄 수 있습니다.\n",
        "        indexes = list()\n",
        "        for word in self.tokenizer(sentence):\n",
        "          indexes.append(self.word2index[word])\n",
        "            # try:\n",
        "                # indexes.append(self.word2index[word])\n",
        "            # except KeyError:\n",
        "                # indexes.append(UNK_token)\n",
        "\n",
        "        indexes.append(EOS_token)\n",
        "        len_sen = len(indexes)\n",
        "        if len_sen > max_len:\n",
        "            indexes = indexes[:max_len-1]\n",
        "            indexes.append(EOS_token)\n",
        "            len_sen = max_len\n",
        "\n",
        "        index_tensor = torch.tensor(indexes)\n",
        "        return index_tensor, len_sen"
      ],
      "metadata": {
        "id": "1pknRAocIsY6"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYucrg5wCjyA"
      },
      "source": [
        "def train(model, device, optimizer, train_loader, num_epochs):\n",
        "    running_loss = 0.0\n",
        "    best_loss = 0.0\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for input_tensor, input_length, target_tensor, target_length in train_loader:\n",
        "            input_tensor = input_tensor[0].to(device)\n",
        "            target_tensor = target_tensor[0].to(device)\n",
        "\n",
        "            loss_sum = model(input_tensor, input_length, target_tensor, target_length)\n",
        "            optimizer.zero_grad()\n",
        "            loss_sum.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss_sum.item() / target_length.item()\n",
        "            if running_loss < best_loss:\n",
        "              print(\"best_loss updated\")\n",
        "              torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/model.pt')\n",
        "              best_loss = running_loss\n",
        "        print('Epoch {}, Train Loss: {:.4f}'.format(epoch + 1, running_loss))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "on_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2\n",
        "\n",
        "max_len = 10\n",
        "hidden_size = 256\n",
        "\n",
        "eng_dic, kor_dic = make_dic(train_data)\n",
        "\n",
        "eng_dic_size = eng_dic.n_words\n",
        "kor_dic_size = kor_dic.n_words\n",
        "\n",
        "train_loader = make_data_loader(train_data, eng_dic, kor_dic, max_len, 1)"
      ],
      "metadata": {
        "id": "UQONZ4ZlKfka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GRUMT(eng_dic_size, hidden_size, kor_dic_size, max_len, on_device).to(on_device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "6XYnFK71P8QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, on_device, optimizer, train_loader, 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "FpoEahsjP-KO",
        "outputId": "572e659d-8951-43e5-f39b-394730215ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 1839.3313\n",
            "Epoch 2, Train Loss: 3290.3877\n",
            "Epoch 3, Train Loss: 4689.4274\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b47dbcb65b67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-6bcfbcc61309>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, optimizer, train_loader, num_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mloss_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mloss_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-ff4af05b8526>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, input_length, target_tensor, target_length)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mloss_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-ff4af05b8526>\u001b[0m in \u001b[0;36m_decoder\u001b[0;34m(self, target_tensor, target_length, encoder_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mdecoder_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mattn_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz8Q3e9iQroL"
      },
      "source": [
        "## evaluate 함수\n",
        "\n",
        "해당 함수에서는 validation data를 이용하여 학습된 `model`을 평가합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PcidZwmQuFu"
      },
      "source": [
        "def evaluate(model, device, valid_loader):\n",
        "    # 학습 중 모델을 평가합니다.\n",
        "    # 모델에게 학습이 아닌 평가를 할 것이라고 알립니다.\n",
        "    model.eval()\n",
        "    valid_running_loss = 0.0\n",
        "\n",
        "    # 학습이 아니기에 최적화를 하지 않는다는 환경을 설정합니다.\n",
        "    with torch.no_grad():\n",
        "        # validation 데이터를 읽습니다.\n",
        "        for input_tensor, input_length, target_tensor, target_length in valid_loader:\n",
        "            input_tensor = input_tensor[0].to(device)\n",
        "            target_tensor = target_tensor[0].to(device)\n",
        "\n",
        "            # model을 함수처럼 호출하면 model에서 정의한 forward 함수가 실행됩니다.\n",
        "            # 즉, 데이터를 모델에 집어넣어 forward방향으로 흐른 후 그 결과를 받습니다.\n",
        "            loss_sum = model(input_tensor, input_length, target_tensor, target_length)\n",
        "\n",
        "            # validation 데이터의 loss, 즉 모델의 출력과 실제 데이터의 차이를 구합니다.\n",
        "            valid_running_loss += loss_sum.item() / target_length.item()\n",
        "\n",
        "    # 평균 loss를 계산하여 반환합니다.\n",
        "    return valid_running_loss / len(valid_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra8NgFTOCy-p"
      },
      "source": [
        "## 데이터 불러오기\n",
        "\n",
        "데이터를 불러올 때 우리는 먼저 언어별 사전을 정의하고 그 사전에 맞춰 데이터를 불러옵니다.\n",
        "\n",
        "- 문제 2. `valid_loader`를 불러오세요. 힌트) `train_loader`을 참고하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jy8JWuPCyvC"
      },
      "source": [
        "on_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# on_device = torch.device('cpu')\n",
        "\n",
        "# 사전 내 특수 단어의 index를 각각 미리 정의합니다.\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2\n",
        "\n",
        "# 데이터의 파일 정보\n",
        "train_file_path = \"./train_en_ko.csv\"\n",
        "valid_file_path = \"./valid_en_ko.csv\"\n",
        "\n",
        "max_len = 10\n",
        "hidden_size = 256\n",
        "\n",
        "# 언어 별 사전 생성\n",
        "eng_dic, kor_dic = make_dic(dataset_path=train_file_path)\n",
        "\n",
        "#  train, validation 데이터 csv 파일을 읽어옵니다.\n",
        "train_loader = make_data_loader(train_file_path, eng_dic, kor_dic, max_len, 1)\n",
        "\n",
        "# <ToDo>: valid_dataset을 불러오세요.\n",
        "valid_loader = make_data_loader(valid_file_path, eng_dic, kor_dic, max_len, 1)\n",
        "\n",
        "# 영어와 한국어 사전의 크기(단어 개수)를 가져옵니다.\n",
        "eng_dic_size = eng_dic.n_words\n",
        "kor_dic_size = kor_dic.n_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNlcKZKoDDEq"
      },
      "source": [
        "## 모델 학습\n",
        "\n",
        "- 문제 3. `GRUMT` 인스턴스를 만드세요.\n",
        "- 문제 4. `train` 함수를 이용하여 train data를 통해 모델 학습을 진행하세요."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <ToDo>: GRUMT 클래스의 인스턴스를 만드세요. 인스턴스 생성 시 필요한 parameter를 전달해주세요.\n",
        "model = GRUMT(eng_dic_size, hidden_size, kor_dic_size, max_len, on_device).to(on_device)"
      ],
      "metadata": {
        "id": "tYTjso7XUszi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpjT1fyTDF15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d805c8-89a8-40c8-b8c8-1e620a5a002d"
      },
      "source": [
        "# Adam optimizier를 사용합니다.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# <ToDo>: 학습을 위해 train 함수의 적절한 parameter를 전달해주세요.\n",
        "train(model, on_device, optimizer, train_loader, valid_loader, 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Step 100, Train Loss: 4.3022, Valid Loss: 3.3302\n",
            "Epoch 1, Step 200, Train Loss: 3.8001, Valid Loss: 3.7074\n",
            "Epoch 1, Step 300, Train Loss: 3.8735, Valid Loss: 3.5491\n",
            "Epoch 1, Step 400, Train Loss: 4.2952, Valid Loss: 4.3542\n",
            "Epoch 1, Step 500, Train Loss: 4.1805, Valid Loss: 4.7959\n",
            "Epoch 1, Step 600, Train Loss: 4.0673, Valid Loss: 3.3506\n",
            "Epoch 1, Step 700, Train Loss: 4.2876, Valid Loss: 4.0127\n",
            "Epoch 1, Step 800, Train Loss: 4.2454, Valid Loss: 3.8350\n",
            "Epoch 1, Step 900, Train Loss: 3.9232, Valid Loss: 3.2819\n",
            "Epoch 1, Step 1000, Train Loss: 4.0964, Valid Loss: 2.6605\n",
            "Epoch 1, Step 1100, Train Loss: 3.8758, Valid Loss: 4.3409\n",
            "Epoch 1, Step 1200, Train Loss: 4.2298, Valid Loss: 3.4893\n",
            "Epoch 1, Step 1300, Train Loss: 3.8367, Valid Loss: 2.6529\n",
            "Epoch 1, Step 1400, Train Loss: 4.0012, Valid Loss: 3.4456\n",
            "Epoch 1, Step 1500, Train Loss: 3.9890, Valid Loss: 3.0271\n",
            "Epoch 1, Step 1600, Train Loss: 3.7667, Valid Loss: 3.1136\n",
            "Epoch 1, Step 1700, Train Loss: 4.2531, Valid Loss: 4.5205\n",
            "Epoch 1, Step 1800, Train Loss: 4.3580, Valid Loss: 3.5806\n",
            "Epoch 1, Step 1900, Train Loss: 4.0116, Valid Loss: 4.0693\n",
            "Epoch 1, Step 2000, Train Loss: 4.5454, Valid Loss: 3.5795\n",
            "Epoch 2, Step 2100, Train Loss: 3.4330, Valid Loss: 3.6242\n",
            "Epoch 2, Step 2200, Train Loss: 3.7355, Valid Loss: 4.3614\n",
            "Epoch 2, Step 2300, Train Loss: 3.4956, Valid Loss: 3.5818\n",
            "Epoch 2, Step 2400, Train Loss: 3.5274, Valid Loss: 4.3978\n",
            "Epoch 2, Step 2500, Train Loss: 3.6798, Valid Loss: 5.1837\n",
            "Epoch 2, Step 2600, Train Loss: 3.6002, Valid Loss: 4.4362\n",
            "Epoch 2, Step 2700, Train Loss: 3.2670, Valid Loss: 4.7650\n",
            "Epoch 2, Step 2800, Train Loss: 3.4738, Valid Loss: 4.3064\n",
            "Epoch 2, Step 2900, Train Loss: 3.8470, Valid Loss: 3.9331\n",
            "Epoch 2, Step 3000, Train Loss: 3.2379, Valid Loss: 3.1290\n",
            "Epoch 2, Step 3100, Train Loss: 3.4954, Valid Loss: 4.1090\n",
            "Epoch 2, Step 3200, Train Loss: 3.5342, Valid Loss: 3.8541\n",
            "Epoch 2, Step 3300, Train Loss: 3.3288, Valid Loss: 3.4021\n",
            "Epoch 2, Step 3400, Train Loss: 3.1579, Valid Loss: 4.1531\n",
            "Epoch 2, Step 3500, Train Loss: 3.2770, Valid Loss: 3.5215\n",
            "Epoch 2, Step 3600, Train Loss: 3.4259, Valid Loss: 4.1299\n",
            "Epoch 2, Step 3700, Train Loss: 3.6571, Valid Loss: 4.7552\n",
            "Epoch 2, Step 3800, Train Loss: 3.3483, Valid Loss: 3.8445\n",
            "Epoch 2, Step 3900, Train Loss: 3.3669, Valid Loss: 4.1524\n",
            "Epoch 2, Step 4000, Train Loss: 3.5486, Valid Loss: 3.8234\n",
            "Epoch 3, Step 4100, Train Loss: 2.9950, Valid Loss: 3.9668\n",
            "Epoch 3, Step 4200, Train Loss: 3.1888, Valid Loss: 4.2931\n",
            "Epoch 3, Step 4300, Train Loss: 3.0448, Valid Loss: 3.7704\n",
            "Epoch 3, Step 4400, Train Loss: 3.0388, Valid Loss: 4.0097\n",
            "Epoch 3, Step 4500, Train Loss: 2.6330, Valid Loss: 5.3478\n",
            "Epoch 3, Step 4600, Train Loss: 2.9540, Valid Loss: 3.7777\n",
            "Epoch 3, Step 4700, Train Loss: 2.5767, Valid Loss: 5.3309\n",
            "Epoch 3, Step 4800, Train Loss: 2.9412, Valid Loss: 4.3989\n",
            "Epoch 3, Step 4900, Train Loss: 2.8368, Valid Loss: 3.7771\n",
            "Epoch 3, Step 5000, Train Loss: 2.7522, Valid Loss: 3.5287\n",
            "Epoch 3, Step 5100, Train Loss: 2.7779, Valid Loss: 3.8156\n",
            "Epoch 3, Step 5200, Train Loss: 2.6892, Valid Loss: 3.7679\n",
            "Epoch 3, Step 5300, Train Loss: 2.5943, Valid Loss: 3.4982\n",
            "Epoch 3, Step 5400, Train Loss: 2.4958, Valid Loss: 5.1018\n",
            "Epoch 3, Step 5500, Train Loss: 2.5952, Valid Loss: 3.9830\n",
            "Epoch 3, Step 5600, Train Loss: 2.8867, Valid Loss: 4.4498\n",
            "Epoch 3, Step 5700, Train Loss: 3.0547, Valid Loss: 4.7297\n",
            "Epoch 3, Step 5800, Train Loss: 3.0708, Valid Loss: 3.7553\n",
            "Epoch 3, Step 5900, Train Loss: 2.6966, Valid Loss: 4.2511\n",
            "Epoch 3, Step 6000, Train Loss: 2.5981, Valid Loss: 4.4813\n",
            "Epoch 4, Step 6100, Train Loss: 2.4448, Valid Loss: 4.5790\n",
            "Epoch 4, Step 6200, Train Loss: 2.8050, Valid Loss: 4.4833\n",
            "Epoch 4, Step 6300, Train Loss: 2.4637, Valid Loss: 3.9025\n",
            "Epoch 4, Step 6400, Train Loss: 2.4536, Valid Loss: 4.2739\n",
            "Epoch 4, Step 6500, Train Loss: 2.0452, Valid Loss: 4.6346\n",
            "Epoch 4, Step 6600, Train Loss: 2.4968, Valid Loss: 4.6861\n",
            "Epoch 4, Step 6700, Train Loss: 2.0946, Valid Loss: 5.3458\n",
            "Epoch 4, Step 6800, Train Loss: 2.5454, Valid Loss: 4.8073\n",
            "Epoch 4, Step 6900, Train Loss: 2.3041, Valid Loss: 4.7028\n",
            "Epoch 4, Step 7000, Train Loss: 2.1943, Valid Loss: 3.8157\n",
            "Epoch 4, Step 7100, Train Loss: 1.9285, Valid Loss: 4.6455\n",
            "Epoch 4, Step 7200, Train Loss: 2.2023, Valid Loss: 4.4620\n",
            "Epoch 4, Step 7300, Train Loss: 1.8463, Valid Loss: 4.1405\n",
            "Epoch 4, Step 7400, Train Loss: 1.9494, Valid Loss: 4.5786\n",
            "Epoch 4, Step 7500, Train Loss: 1.9149, Valid Loss: 4.7667\n",
            "Epoch 4, Step 7600, Train Loss: 2.1173, Valid Loss: 4.6185\n",
            "Epoch 4, Step 7700, Train Loss: 2.3218, Valid Loss: 4.7832\n",
            "Epoch 4, Step 7800, Train Loss: 2.5746, Valid Loss: 5.1968\n",
            "Epoch 4, Step 7900, Train Loss: 2.2805, Valid Loss: 5.0209\n",
            "Epoch 4, Step 8000, Train Loss: 2.0930, Valid Loss: 4.9460\n",
            "Epoch 5, Step 8100, Train Loss: 1.8905, Valid Loss: 4.4009\n",
            "Epoch 5, Step 8200, Train Loss: 2.1817, Valid Loss: 5.1005\n",
            "Epoch 5, Step 8300, Train Loss: 2.3319, Valid Loss: 4.8231\n",
            "Epoch 5, Step 8400, Train Loss: 1.8940, Valid Loss: 4.8498\n",
            "Epoch 5, Step 8500, Train Loss: 1.7640, Valid Loss: 5.5699\n",
            "Epoch 5, Step 8600, Train Loss: 2.0047, Valid Loss: 4.8244\n",
            "Epoch 5, Step 8700, Train Loss: 1.5711, Valid Loss: 5.1040\n",
            "Epoch 5, Step 8800, Train Loss: 2.0987, Valid Loss: 5.1895\n",
            "Epoch 5, Step 8900, Train Loss: 1.8181, Valid Loss: 4.7893\n",
            "Epoch 5, Step 9000, Train Loss: 1.6454, Valid Loss: 4.4825\n",
            "Epoch 5, Step 9100, Train Loss: 1.4254, Valid Loss: 4.6854\n",
            "Epoch 5, Step 9200, Train Loss: 1.6929, Valid Loss: 5.1246\n",
            "Epoch 5, Step 9300, Train Loss: 1.3641, Valid Loss: 4.5258\n",
            "Epoch 5, Step 9400, Train Loss: 1.6233, Valid Loss: 4.7460\n",
            "Epoch 5, Step 9500, Train Loss: 1.5476, Valid Loss: 5.0150\n",
            "Epoch 5, Step 9600, Train Loss: 1.7483, Valid Loss: 5.1838\n",
            "Epoch 5, Step 9700, Train Loss: 1.9288, Valid Loss: 5.2085\n",
            "Epoch 5, Step 9800, Train Loss: 1.8727, Valid Loss: 5.0067\n",
            "Epoch 5, Step 9900, Train Loss: 1.6723, Valid Loss: 5.3347\n",
            "Epoch 5, Step 10000, Train Loss: 1.8445, Valid Loss: 5.2247\n",
            "Epoch 6, Step 10100, Train Loss: 1.6061, Valid Loss: 4.7199\n",
            "Epoch 6, Step 10200, Train Loss: 1.8000, Valid Loss: 5.1237\n",
            "Epoch 6, Step 10300, Train Loss: 1.9095, Valid Loss: 5.2509\n",
            "Epoch 6, Step 10400, Train Loss: 1.9664, Valid Loss: 5.3091\n",
            "Epoch 6, Step 10500, Train Loss: 1.6736, Valid Loss: 5.3754\n",
            "Epoch 6, Step 10600, Train Loss: 1.7424, Valid Loss: 5.1528\n",
            "Epoch 6, Step 10700, Train Loss: 1.4140, Valid Loss: 4.9928\n",
            "Epoch 6, Step 10800, Train Loss: 1.6966, Valid Loss: 5.2604\n",
            "Epoch 6, Step 10900, Train Loss: 1.6959, Valid Loss: 4.7517\n",
            "Epoch 6, Step 11000, Train Loss: 1.2959, Valid Loss: 4.8284\n",
            "Epoch 6, Step 11100, Train Loss: 1.2454, Valid Loss: 4.8949\n",
            "Epoch 6, Step 11200, Train Loss: 1.3165, Valid Loss: 5.2065\n",
            "Epoch 6, Step 11300, Train Loss: 1.2167, Valid Loss: 5.0386\n",
            "Epoch 6, Step 11400, Train Loss: 1.3643, Valid Loss: 5.2657\n",
            "Epoch 6, Step 11500, Train Loss: 1.3813, Valid Loss: 5.0270\n",
            "Epoch 6, Step 11600, Train Loss: 1.3958, Valid Loss: 5.1888\n",
            "Epoch 6, Step 11700, Train Loss: 1.6777, Valid Loss: 5.6600\n",
            "Epoch 6, Step 11800, Train Loss: 1.5896, Valid Loss: 5.4667\n",
            "Epoch 6, Step 11900, Train Loss: 1.8020, Valid Loss: 5.4208\n",
            "Epoch 6, Step 12000, Train Loss: 1.6553, Valid Loss: 5.2020\n",
            "Epoch 7, Step 12100, Train Loss: 1.4032, Valid Loss: 5.0564\n",
            "Epoch 7, Step 12200, Train Loss: 1.7238, Valid Loss: 5.4249\n",
            "Epoch 7, Step 12300, Train Loss: 1.5651, Valid Loss: 5.0455\n",
            "Epoch 7, Step 12400, Train Loss: 1.6161, Valid Loss: 5.6278\n",
            "Epoch 7, Step 12500, Train Loss: 1.5401, Valid Loss: 5.4922\n",
            "Epoch 7, Step 12600, Train Loss: 1.6149, Valid Loss: 5.3215\n",
            "Epoch 7, Step 12700, Train Loss: 1.4419, Valid Loss: 5.2625\n",
            "Epoch 7, Step 12800, Train Loss: 1.5390, Valid Loss: 5.5742\n",
            "Epoch 7, Step 12900, Train Loss: 1.4523, Valid Loss: 4.9767\n",
            "Epoch 7, Step 13000, Train Loss: 1.3105, Valid Loss: 5.1173\n",
            "Epoch 7, Step 13100, Train Loss: 1.1055, Valid Loss: 5.0182\n",
            "Epoch 7, Step 13200, Train Loss: 1.1995, Valid Loss: 5.3095\n",
            "Epoch 7, Step 13300, Train Loss: 1.0245, Valid Loss: 4.9296\n",
            "Epoch 7, Step 13400, Train Loss: 1.1951, Valid Loss: 5.2529\n",
            "Epoch 7, Step 13500, Train Loss: 1.2660, Valid Loss: 5.1651\n",
            "Epoch 7, Step 13600, Train Loss: 1.4027, Valid Loss: 5.4361\n",
            "Epoch 7, Step 13700, Train Loss: 1.6236, Valid Loss: 5.9289\n",
            "Epoch 7, Step 13800, Train Loss: 1.5394, Valid Loss: 5.8522\n",
            "Epoch 7, Step 13900, Train Loss: 1.5818, Valid Loss: 5.3164\n",
            "Epoch 7, Step 14000, Train Loss: 1.5185, Valid Loss: 5.3790\n",
            "Epoch 8, Step 14100, Train Loss: 1.2418, Valid Loss: 5.3564\n",
            "Epoch 8, Step 14200, Train Loss: 1.5851, Valid Loss: 5.4559\n",
            "Epoch 8, Step 14300, Train Loss: 1.5547, Valid Loss: 5.3834\n",
            "Epoch 8, Step 14400, Train Loss: 1.3753, Valid Loss: 5.7063\n",
            "Epoch 8, Step 14500, Train Loss: 1.5932, Valid Loss: 5.5233\n",
            "Epoch 8, Step 14600, Train Loss: 1.5227, Valid Loss: 5.6386\n",
            "Epoch 8, Step 14700, Train Loss: 1.3167, Valid Loss: 5.2621\n",
            "Epoch 8, Step 14800, Train Loss: 1.3858, Valid Loss: 5.4150\n",
            "Epoch 8, Step 14900, Train Loss: 1.3311, Valid Loss: 5.2475\n",
            "Epoch 8, Step 15000, Train Loss: 1.1456, Valid Loss: 5.1966\n",
            "Epoch 8, Step 15100, Train Loss: 0.9962, Valid Loss: 5.2129\n",
            "Epoch 8, Step 15200, Train Loss: 1.2002, Valid Loss: 5.5285\n",
            "Epoch 8, Step 15300, Train Loss: 0.9017, Valid Loss: 5.3198\n",
            "Epoch 8, Step 15400, Train Loss: 1.1123, Valid Loss: 5.5298\n",
            "Epoch 8, Step 15500, Train Loss: 1.1379, Valid Loss: 5.6020\n",
            "Epoch 8, Step 15600, Train Loss: 1.2458, Valid Loss: 5.3080\n",
            "Epoch 8, Step 15700, Train Loss: 1.4412, Valid Loss: 5.6430\n",
            "Epoch 8, Step 15800, Train Loss: 1.3572, Valid Loss: 6.1673\n",
            "Epoch 8, Step 15900, Train Loss: 1.3525, Valid Loss: 5.5005\n",
            "Epoch 8, Step 16000, Train Loss: 1.2675, Valid Loss: 5.6546\n",
            "Epoch 9, Step 16100, Train Loss: 1.0396, Valid Loss: 5.3059\n",
            "Epoch 9, Step 16200, Train Loss: 1.3777, Valid Loss: 5.6441\n",
            "Epoch 9, Step 16300, Train Loss: 1.3617, Valid Loss: 5.6474\n",
            "Epoch 9, Step 16400, Train Loss: 1.3582, Valid Loss: 5.7775\n",
            "Epoch 9, Step 16500, Train Loss: 1.2622, Valid Loss: 5.8022\n",
            "Epoch 9, Step 16600, Train Loss: 1.2870, Valid Loss: 5.8116\n",
            "Epoch 9, Step 16700, Train Loss: 1.1491, Valid Loss: 5.5786\n",
            "Epoch 9, Step 16800, Train Loss: 1.3307, Valid Loss: 5.4439\n",
            "Epoch 9, Step 16900, Train Loss: 1.2117, Valid Loss: 5.4267\n",
            "Epoch 9, Step 17000, Train Loss: 1.0495, Valid Loss: 5.4768\n",
            "Epoch 9, Step 17100, Train Loss: 0.9448, Valid Loss: 5.4089\n",
            "Epoch 9, Step 17200, Train Loss: 1.0848, Valid Loss: 5.6253\n",
            "Epoch 9, Step 17300, Train Loss: 0.8111, Valid Loss: 5.5985\n",
            "Epoch 9, Step 17400, Train Loss: 0.9364, Valid Loss: 5.8049\n",
            "Epoch 9, Step 17500, Train Loss: 1.0360, Valid Loss: 5.7276\n",
            "Epoch 9, Step 17600, Train Loss: 1.1422, Valid Loss: 5.7587\n",
            "Epoch 9, Step 17700, Train Loss: 1.3071, Valid Loss: 5.9013\n",
            "Epoch 9, Step 17800, Train Loss: 1.1204, Valid Loss: 5.8533\n",
            "Epoch 9, Step 17900, Train Loss: 1.2702, Valid Loss: 5.6510\n",
            "Epoch 9, Step 18000, Train Loss: 1.2379, Valid Loss: 5.8850\n",
            "Epoch 10, Step 18100, Train Loss: 1.0219, Valid Loss: 5.7833\n",
            "Epoch 10, Step 18200, Train Loss: 1.2720, Valid Loss: 5.9340\n",
            "Epoch 10, Step 18300, Train Loss: 1.2468, Valid Loss: 5.7352\n",
            "Epoch 10, Step 18400, Train Loss: 1.1300, Valid Loss: 6.0060\n",
            "Epoch 10, Step 18500, Train Loss: 1.1156, Valid Loss: 5.9177\n",
            "Epoch 10, Step 18600, Train Loss: 1.3529, Valid Loss: 6.0371\n",
            "Epoch 10, Step 18700, Train Loss: 1.0073, Valid Loss: 6.0380\n",
            "Epoch 10, Step 18800, Train Loss: 1.2120, Valid Loss: 6.1318\n",
            "Epoch 10, Step 18900, Train Loss: 0.9941, Valid Loss: 5.8195\n",
            "Epoch 10, Step 19000, Train Loss: 0.8890, Valid Loss: 5.8832\n",
            "Epoch 10, Step 19100, Train Loss: 0.8454, Valid Loss: 5.6710\n",
            "Epoch 10, Step 19200, Train Loss: 0.9988, Valid Loss: 5.7499\n",
            "Epoch 10, Step 19300, Train Loss: 0.7266, Valid Loss: 5.9020\n",
            "Epoch 10, Step 19400, Train Loss: 0.9208, Valid Loss: 5.8435\n",
            "Epoch 10, Step 19500, Train Loss: 0.8454, Valid Loss: 6.0873\n",
            "Epoch 10, Step 19600, Train Loss: 1.0407, Valid Loss: 5.8209\n",
            "Epoch 10, Step 19700, Train Loss: 1.2805, Valid Loss: 6.1173\n",
            "Epoch 10, Step 19800, Train Loss: 1.1437, Valid Loss: 6.2543\n",
            "Epoch 10, Step 19900, Train Loss: 1.1498, Valid Loss: 6.1177\n",
            "Epoch 10, Step 20000, Train Loss: 1.0735, Valid Loss: 5.9571\n",
            "Epoch 11, Step 20100, Train Loss: 0.9528, Valid Loss: 5.9641\n",
            "Epoch 11, Step 20200, Train Loss: 1.0674, Valid Loss: 5.9587\n",
            "Epoch 11, Step 20300, Train Loss: 1.2185, Valid Loss: 6.0050\n",
            "Epoch 11, Step 20400, Train Loss: 1.0908, Valid Loss: 6.2412\n",
            "Epoch 11, Step 20500, Train Loss: 1.0196, Valid Loss: 6.2737\n",
            "Epoch 11, Step 20600, Train Loss: 1.1228, Valid Loss: 6.1510\n",
            "Epoch 11, Step 20700, Train Loss: 0.9659, Valid Loss: 6.1960\n",
            "Epoch 11, Step 20800, Train Loss: 1.1545, Valid Loss: 6.1855\n",
            "Epoch 11, Step 20900, Train Loss: 1.0264, Valid Loss: 6.0198\n",
            "Epoch 11, Step 21000, Train Loss: 0.8949, Valid Loss: 6.0597\n",
            "Epoch 11, Step 21100, Train Loss: 0.7371, Valid Loss: 5.8651\n",
            "Epoch 11, Step 21200, Train Loss: 0.8627, Valid Loss: 6.2180\n",
            "Epoch 11, Step 21300, Train Loss: 0.6900, Valid Loss: 6.0737\n",
            "Epoch 11, Step 21400, Train Loss: 0.7806, Valid Loss: 5.9504\n",
            "Epoch 11, Step 21500, Train Loss: 0.7855, Valid Loss: 6.0670\n",
            "Epoch 11, Step 21600, Train Loss: 0.9652, Valid Loss: 6.1402\n",
            "Epoch 11, Step 21700, Train Loss: 1.1001, Valid Loss: 6.2056\n",
            "Epoch 11, Step 21800, Train Loss: 1.0625, Valid Loss: 6.4147\n",
            "Epoch 11, Step 21900, Train Loss: 0.9871, Valid Loss: 6.0587\n",
            "Epoch 11, Step 22000, Train Loss: 0.9961, Valid Loss: 6.2077\n",
            "Epoch 12, Step 22100, Train Loss: 0.8826, Valid Loss: 6.2000\n",
            "Epoch 12, Step 22200, Train Loss: 1.1260, Valid Loss: 6.0276\n",
            "Epoch 12, Step 22300, Train Loss: 1.1688, Valid Loss: 6.0416\n",
            "Epoch 12, Step 22400, Train Loss: 1.0520, Valid Loss: 6.3192\n",
            "Epoch 12, Step 22500, Train Loss: 1.0506, Valid Loss: 6.3911\n",
            "Epoch 12, Step 22600, Train Loss: 1.1073, Valid Loss: 6.3606\n",
            "Epoch 12, Step 22700, Train Loss: 0.9189, Valid Loss: 6.1136\n",
            "Epoch 12, Step 22800, Train Loss: 1.0670, Valid Loss: 6.4768\n",
            "Epoch 12, Step 22900, Train Loss: 0.9263, Valid Loss: 6.4354\n",
            "Epoch 12, Step 23000, Train Loss: 0.7764, Valid Loss: 6.2665\n",
            "Epoch 12, Step 23100, Train Loss: 0.7218, Valid Loss: 5.9401\n",
            "Epoch 12, Step 23200, Train Loss: 0.8367, Valid Loss: 6.1739\n",
            "Epoch 12, Step 23300, Train Loss: 0.6392, Valid Loss: 6.0930\n",
            "Epoch 12, Step 23400, Train Loss: 0.7572, Valid Loss: 6.3612\n",
            "Epoch 12, Step 23500, Train Loss: 0.8183, Valid Loss: 6.0501\n",
            "Epoch 12, Step 23600, Train Loss: 0.9185, Valid Loss: 6.0718\n",
            "Epoch 12, Step 23700, Train Loss: 1.1073, Valid Loss: 6.2137\n",
            "Epoch 12, Step 23800, Train Loss: 0.9425, Valid Loss: 6.4619\n",
            "Epoch 12, Step 23900, Train Loss: 0.9177, Valid Loss: 6.4118\n",
            "Epoch 12, Step 24000, Train Loss: 1.1112, Valid Loss: 6.3873\n",
            "Epoch 13, Step 24100, Train Loss: 0.8755, Valid Loss: 6.1533\n",
            "Epoch 13, Step 24200, Train Loss: 1.1410, Valid Loss: 6.1111\n",
            "Epoch 13, Step 24300, Train Loss: 1.0781, Valid Loss: 6.0968\n",
            "Epoch 13, Step 24400, Train Loss: 0.9044, Valid Loss: 6.1479\n",
            "Epoch 13, Step 24500, Train Loss: 0.9339, Valid Loss: 6.1006\n",
            "Epoch 13, Step 24600, Train Loss: 1.1143, Valid Loss: 6.2037\n",
            "Epoch 13, Step 24700, Train Loss: 0.8673, Valid Loss: 6.4102\n",
            "Epoch 13, Step 24800, Train Loss: 0.9335, Valid Loss: 6.4044\n",
            "Epoch 13, Step 24900, Train Loss: 0.9122, Valid Loss: 6.0974\n",
            "Epoch 13, Step 25000, Train Loss: 0.7322, Valid Loss: 6.0821\n",
            "Epoch 13, Step 25100, Train Loss: 0.6225, Valid Loss: 6.0660\n",
            "Epoch 13, Step 25200, Train Loss: 0.7582, Valid Loss: 6.1898\n",
            "Epoch 13, Step 25300, Train Loss: 0.5403, Valid Loss: 6.3038\n",
            "Epoch 13, Step 25400, Train Loss: 0.8226, Valid Loss: 6.4409\n",
            "Epoch 13, Step 25500, Train Loss: 0.7304, Valid Loss: 6.3863\n",
            "Epoch 13, Step 25600, Train Loss: 0.8510, Valid Loss: 6.1151\n",
            "Epoch 13, Step 25700, Train Loss: 0.8941, Valid Loss: 6.5349\n",
            "Epoch 13, Step 25800, Train Loss: 0.9462, Valid Loss: 6.6550\n",
            "Epoch 13, Step 25900, Train Loss: 0.8188, Valid Loss: 6.5980\n",
            "Epoch 13, Step 26000, Train Loss: 0.9465, Valid Loss: 6.6735\n",
            "Epoch 14, Step 26100, Train Loss: 0.6923, Valid Loss: 6.5291\n",
            "Epoch 14, Step 26200, Train Loss: 0.9692, Valid Loss: 6.3110\n",
            "Epoch 14, Step 26300, Train Loss: 0.9966, Valid Loss: 6.3473\n",
            "Epoch 14, Step 26400, Train Loss: 0.8975, Valid Loss: 6.3905\n",
            "Epoch 14, Step 26500, Train Loss: 0.9477, Valid Loss: 6.4433\n",
            "Epoch 14, Step 26600, Train Loss: 1.0205, Valid Loss: 6.3425\n",
            "Epoch 14, Step 26700, Train Loss: 0.8348, Valid Loss: 6.3115\n",
            "Epoch 14, Step 26800, Train Loss: 0.9365, Valid Loss: 6.5176\n",
            "Epoch 14, Step 26900, Train Loss: 0.8633, Valid Loss: 6.4348\n",
            "Epoch 14, Step 27000, Train Loss: 0.7244, Valid Loss: 6.3191\n",
            "Epoch 14, Step 27100, Train Loss: 0.5836, Valid Loss: 6.1585\n",
            "Epoch 14, Step 27200, Train Loss: 0.7291, Valid Loss: 6.2158\n",
            "Epoch 14, Step 27300, Train Loss: 0.4983, Valid Loss: 6.5641\n",
            "Epoch 14, Step 27400, Train Loss: 0.6965, Valid Loss: 6.2943\n",
            "Epoch 14, Step 27500, Train Loss: 0.6350, Valid Loss: 6.3779\n",
            "Epoch 14, Step 27600, Train Loss: 0.8604, Valid Loss: 6.4031\n",
            "Epoch 14, Step 27700, Train Loss: 0.9213, Valid Loss: 6.5535\n",
            "Epoch 14, Step 27800, Train Loss: 0.8614, Valid Loss: 6.7333\n",
            "Epoch 14, Step 27900, Train Loss: 0.7875, Valid Loss: 6.6104\n",
            "Epoch 14, Step 28000, Train Loss: 0.7865, Valid Loss: 6.5811\n",
            "Epoch 15, Step 28100, Train Loss: 0.6847, Valid Loss: 6.7488\n",
            "Epoch 15, Step 28200, Train Loss: 0.9802, Valid Loss: 6.5860\n",
            "Epoch 15, Step 28300, Train Loss: 1.0137, Valid Loss: 6.5924\n",
            "Epoch 15, Step 28400, Train Loss: 0.9229, Valid Loss: 6.5937\n",
            "Epoch 15, Step 28500, Train Loss: 0.8822, Valid Loss: 6.4872\n",
            "Epoch 15, Step 28600, Train Loss: 0.9940, Valid Loss: 6.5408\n",
            "Epoch 15, Step 28700, Train Loss: 0.8743, Valid Loss: 6.5261\n",
            "Epoch 15, Step 28800, Train Loss: 0.8686, Valid Loss: 6.7256\n",
            "Epoch 15, Step 28900, Train Loss: 0.7638, Valid Loss: 6.6360\n",
            "Epoch 15, Step 29000, Train Loss: 0.6703, Valid Loss: 6.5743\n",
            "Epoch 15, Step 29100, Train Loss: 0.4873, Valid Loss: 6.4370\n",
            "Epoch 15, Step 29200, Train Loss: 0.6607, Valid Loss: 6.7063\n",
            "Epoch 15, Step 29300, Train Loss: 0.4695, Valid Loss: 6.5170\n",
            "Epoch 15, Step 29400, Train Loss: 0.6947, Valid Loss: 6.6755\n",
            "Epoch 15, Step 29500, Train Loss: 0.6230, Valid Loss: 6.6316\n",
            "Epoch 15, Step 29600, Train Loss: 0.7712, Valid Loss: 6.6539\n",
            "Epoch 15, Step 29700, Train Loss: 0.8766, Valid Loss: 6.7885\n",
            "Epoch 15, Step 29800, Train Loss: 0.7760, Valid Loss: 7.0361\n",
            "Epoch 15, Step 29900, Train Loss: 0.7835, Valid Loss: 6.7801\n",
            "Epoch 15, Step 30000, Train Loss: 0.8149, Valid Loss: 6.8111\n",
            "Epoch 16, Step 30100, Train Loss: 0.6457, Valid Loss: 6.8141\n",
            "Epoch 16, Step 30200, Train Loss: 0.8428, Valid Loss: 6.5057\n",
            "Epoch 16, Step 30300, Train Loss: 0.9115, Valid Loss: 6.5668\n",
            "Epoch 16, Step 30400, Train Loss: 0.7529, Valid Loss: 6.7154\n",
            "Epoch 16, Step 30500, Train Loss: 0.7871, Valid Loss: 6.7138\n",
            "Epoch 16, Step 30600, Train Loss: 0.8838, Valid Loss: 6.8368\n",
            "Epoch 16, Step 30700, Train Loss: 0.8454, Valid Loss: 6.6801\n",
            "Epoch 16, Step 30800, Train Loss: 0.8533, Valid Loss: 6.7013\n",
            "Epoch 16, Step 30900, Train Loss: 0.7358, Valid Loss: 6.6411\n",
            "Epoch 16, Step 31000, Train Loss: 0.5947, Valid Loss: 6.6441\n",
            "Epoch 16, Step 31100, Train Loss: 0.5085, Valid Loss: 6.4664\n",
            "Epoch 16, Step 31200, Train Loss: 0.6076, Valid Loss: 6.5178\n",
            "Epoch 16, Step 31300, Train Loss: 0.4180, Valid Loss: 6.7055\n",
            "Epoch 16, Step 31400, Train Loss: 0.6590, Valid Loss: 6.6883\n",
            "Epoch 16, Step 31500, Train Loss: 0.5875, Valid Loss: 6.6198\n",
            "Epoch 16, Step 31600, Train Loss: 0.8767, Valid Loss: 6.5708\n",
            "Epoch 16, Step 31700, Train Loss: 0.8304, Valid Loss: 6.6759\n",
            "Epoch 16, Step 31800, Train Loss: 0.7138, Valid Loss: 7.1448\n",
            "Epoch 16, Step 31900, Train Loss: 0.7308, Valid Loss: 6.9396\n",
            "Epoch 16, Step 32000, Train Loss: 0.8100, Valid Loss: 6.9631\n",
            "Epoch 17, Step 32100, Train Loss: 0.6357, Valid Loss: 6.9797\n",
            "Epoch 17, Step 32200, Train Loss: 0.8077, Valid Loss: 6.8558\n",
            "Epoch 17, Step 32300, Train Loss: 0.9087, Valid Loss: 6.7884\n",
            "Epoch 17, Step 32400, Train Loss: 0.8289, Valid Loss: 6.7779\n",
            "Epoch 17, Step 32500, Train Loss: 0.6684, Valid Loss: 6.7333\n",
            "Epoch 17, Step 32600, Train Loss: 0.9468, Valid Loss: 6.8403\n",
            "Epoch 17, Step 32700, Train Loss: 0.6279, Valid Loss: 6.7767\n",
            "Epoch 17, Step 32800, Train Loss: 0.7944, Valid Loss: 6.9273\n",
            "Epoch 17, Step 32900, Train Loss: 0.6180, Valid Loss: 6.8394\n",
            "Epoch 17, Step 33000, Train Loss: 0.5396, Valid Loss: 6.9374\n",
            "Epoch 17, Step 33100, Train Loss: 0.4852, Valid Loss: 6.7225\n",
            "Epoch 17, Step 33200, Train Loss: 0.5111, Valid Loss: 6.7197\n",
            "Epoch 17, Step 33300, Train Loss: 0.4360, Valid Loss: 6.7403\n",
            "Epoch 17, Step 33400, Train Loss: 0.5962, Valid Loss: 6.6709\n",
            "Epoch 17, Step 33500, Train Loss: 0.5923, Valid Loss: 6.5734\n",
            "Epoch 17, Step 33600, Train Loss: 0.7133, Valid Loss: 6.6364\n",
            "Epoch 17, Step 33700, Train Loss: 0.7513, Valid Loss: 6.8037\n",
            "Epoch 17, Step 33800, Train Loss: 0.6671, Valid Loss: 6.9719\n",
            "Epoch 17, Step 33900, Train Loss: 0.7426, Valid Loss: 7.1450\n",
            "Epoch 17, Step 34000, Train Loss: 0.7378, Valid Loss: 6.9937\n",
            "Epoch 18, Step 34100, Train Loss: 0.5905, Valid Loss: 6.9081\n",
            "Epoch 18, Step 34200, Train Loss: 0.8523, Valid Loss: 6.8380\n",
            "Epoch 18, Step 34300, Train Loss: 0.7173, Valid Loss: 6.7109\n",
            "Epoch 18, Step 34400, Train Loss: 0.7370, Valid Loss: 6.8056\n",
            "Epoch 18, Step 34500, Train Loss: 0.6808, Valid Loss: 6.7476\n",
            "Epoch 18, Step 34600, Train Loss: 0.7845, Valid Loss: 6.7652\n",
            "Epoch 18, Step 34700, Train Loss: 0.6786, Valid Loss: 6.7961\n",
            "Epoch 18, Step 34800, Train Loss: 0.6740, Valid Loss: 6.8082\n",
            "Epoch 18, Step 34900, Train Loss: 0.6603, Valid Loss: 6.9272\n",
            "Epoch 18, Step 35000, Train Loss: 0.5766, Valid Loss: 6.9414\n",
            "Epoch 18, Step 35100, Train Loss: 0.4347, Valid Loss: 6.7554\n",
            "Epoch 18, Step 35200, Train Loss: 0.6106, Valid Loss: 6.5509\n",
            "Epoch 18, Step 35300, Train Loss: 0.3307, Valid Loss: 6.9141\n",
            "Epoch 18, Step 35400, Train Loss: 0.5336, Valid Loss: 6.7627\n",
            "Epoch 18, Step 35500, Train Loss: 0.4855, Valid Loss: 6.8449\n",
            "Epoch 18, Step 35600, Train Loss: 0.6776, Valid Loss: 6.6752\n",
            "Epoch 18, Step 35700, Train Loss: 0.7141, Valid Loss: 6.9356\n",
            "Epoch 18, Step 35800, Train Loss: 0.6413, Valid Loss: 7.0022\n",
            "Epoch 18, Step 35900, Train Loss: 0.7389, Valid Loss: 7.0262\n",
            "Epoch 18, Step 36000, Train Loss: 0.6679, Valid Loss: 7.1428\n",
            "Epoch 19, Step 36100, Train Loss: 0.4952, Valid Loss: 7.2176\n",
            "Epoch 19, Step 36200, Train Loss: 0.7448, Valid Loss: 7.2097\n",
            "Epoch 19, Step 36300, Train Loss: 0.7809, Valid Loss: 7.0044\n",
            "Epoch 19, Step 36400, Train Loss: 0.6816, Valid Loss: 6.9158\n",
            "Epoch 19, Step 36500, Train Loss: 0.5605, Valid Loss: 7.0005\n",
            "Epoch 19, Step 36600, Train Loss: 0.8252, Valid Loss: 6.9803\n",
            "Epoch 19, Step 36700, Train Loss: 0.6223, Valid Loss: 7.0636\n",
            "Epoch 19, Step 36800, Train Loss: 0.7958, Valid Loss: 7.2631\n",
            "Epoch 19, Step 36900, Train Loss: 0.6132, Valid Loss: 7.0600\n",
            "Epoch 19, Step 37000, Train Loss: 0.5532, Valid Loss: 6.9885\n",
            "Epoch 19, Step 37100, Train Loss: 0.3644, Valid Loss: 6.9185\n",
            "Epoch 19, Step 37200, Train Loss: 0.5539, Valid Loss: 6.8439\n",
            "Epoch 19, Step 37300, Train Loss: 0.3756, Valid Loss: 6.9308\n",
            "Epoch 19, Step 37400, Train Loss: 0.4927, Valid Loss: 7.1870\n",
            "Epoch 19, Step 37500, Train Loss: 0.5644, Valid Loss: 6.8987\n",
            "Epoch 19, Step 37600, Train Loss: 0.6698, Valid Loss: 6.7604\n",
            "Epoch 19, Step 37700, Train Loss: 0.5873, Valid Loss: 7.0209\n",
            "Epoch 19, Step 37800, Train Loss: 0.6167, Valid Loss: 7.4381\n",
            "Epoch 19, Step 37900, Train Loss: 0.6435, Valid Loss: 7.2647\n",
            "Epoch 19, Step 38000, Train Loss: 0.6084, Valid Loss: 7.0606\n",
            "Epoch 20, Step 38100, Train Loss: 0.5924, Valid Loss: 7.2005\n",
            "Epoch 20, Step 38200, Train Loss: 0.7514, Valid Loss: 7.2401\n",
            "Epoch 20, Step 38300, Train Loss: 0.6402, Valid Loss: 7.0160\n",
            "Epoch 20, Step 38400, Train Loss: 0.7417, Valid Loss: 7.0561\n",
            "Epoch 20, Step 38500, Train Loss: 0.5799, Valid Loss: 7.0431\n",
            "Epoch 20, Step 38600, Train Loss: 0.7805, Valid Loss: 7.0394\n",
            "Epoch 20, Step 38700, Train Loss: 0.6619, Valid Loss: 7.2549\n",
            "Epoch 20, Step 38800, Train Loss: 0.7315, Valid Loss: 7.1545\n",
            "Epoch 20, Step 38900, Train Loss: 0.6666, Valid Loss: 7.0077\n",
            "Epoch 20, Step 39000, Train Loss: 0.5216, Valid Loss: 7.0657\n",
            "Epoch 20, Step 39100, Train Loss: 0.4206, Valid Loss: 6.9497\n",
            "Epoch 20, Step 39200, Train Loss: 0.4888, Valid Loss: 6.9501\n",
            "Epoch 20, Step 39300, Train Loss: 0.4189, Valid Loss: 6.8932\n",
            "Epoch 20, Step 39400, Train Loss: 0.5128, Valid Loss: 6.9517\n",
            "Epoch 20, Step 39500, Train Loss: 0.5244, Valid Loss: 7.0633\n",
            "Epoch 20, Step 39600, Train Loss: 0.5830, Valid Loss: 6.9454\n",
            "Epoch 20, Step 39700, Train Loss: 0.7423, Valid Loss: 7.0270\n",
            "Epoch 20, Step 39800, Train Loss: 0.5732, Valid Loss: 7.4195\n",
            "Epoch 20, Step 39900, Train Loss: 0.6033, Valid Loss: 7.3649\n",
            "Epoch 20, Step 40000, Train Loss: 0.6140, Valid Loss: 7.4449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Zo7-M8CZJPO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "새로운 학습 데이터가 추가되었을 때 그 성능은 어떻게 변하는지.\n"
      ],
      "metadata": {
        "id": "5EBTZE1-bF96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "다른 스타일의 테스트 데이터인 경우 그 성능은 어떻게 변하는지?"
      ],
      "metadata": {
        "id": "ZpnYAfLJbLev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 질문들에서 문제점이 있다면 이를 어떻게 극복하면 좋을지?"
      ],
      "metadata": {
        "id": "CVtKzSwxbQCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qm2lGKNJbKSJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}